#!/usr/bin/env python3
"""
PhoenixCoder æ€§èƒ½åˆ†æè„šæœ¬
åˆ†ææ€§èƒ½æµ‹è¯•ç»“æœï¼Œç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå’Œå›å½’æ£€æµ‹
"""

import os
import sys
import json
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import logging
import statistics
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from jinja2 import Template
import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, results_dir: str, baseline_dir: str = None):
        self.results_dir = Path(results_dir)
        self.baseline_dir = Path(baseline_dir) if baseline_dir else None
        self.analysis_results = {}
        
    def load_performance_data(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ€§èƒ½æ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½æ€§èƒ½æ•°æ®å¤±è´¥ {file_path}: {e}")
            return None
    
    def load_baseline_data(self, service_name: str, test_type: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½åŸºçº¿æ•°æ®"""
        if not self.baseline_dir or not self.baseline_dir.exists():
            return None
        
        baseline_file = self.baseline_dir / f"{service_name}-{test_type}-baseline.json"
        return self.load_performance_data(baseline_file)
    
    def analyze_backend_performance(self, data: Dict[str, Any], baseline: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ†æåç«¯æ€§èƒ½æ•°æ®"""
        analysis = {
            'service_name': data.get('service_name', 'unknown'),
            'test_type': 'backend',
            'timestamp': data.get('timestamp', ''),
            'environment': data.get('environment', 'unknown'),
            'metrics': {},
            'regression_detected': False,
            'improvements': [],
            'regressions': [],
            'recommendations': []
        }
        
        # åˆ†æå…³é”®æŒ‡æ ‡
        metrics = data.get('metrics', {})
        
        # å“åº”æ—¶é—´åˆ†æ
        response_times = metrics.get('response_times', {})
        if response_times:
            analysis['metrics']['response_time'] = {
                'avg': response_times.get('avg', 0),
                'p50': response_times.get('p50', 0),
                'p95': response_times.get('p95', 0),
                'p99': response_times.get('p99', 0),
                'max': response_times.get('max', 0)
            }
        
        # ååé‡åˆ†æ
        throughput = metrics.get('throughput', {})
        if throughput:
            analysis['metrics']['throughput'] = {
                'rps': throughput.get('requests_per_second', 0),
                'total_requests': throughput.get('total_requests', 0),
                'duration': throughput.get('duration_seconds', 0)
            }
        
        # é”™è¯¯ç‡åˆ†æ
        errors = metrics.get('errors', {})
        if errors:
            total_requests = throughput.get('total_requests', 1)
            error_count = errors.get('total_errors', 0)
            error_rate = (error_count / total_requests) * 100 if total_requests > 0 else 0
            
            analysis['metrics']['error_rate'] = {
                'percentage': error_rate,
                'total_errors': error_count,
                'error_types': errors.get('error_types', {})
            }
        
        # ç³»ç»Ÿèµ„æºåˆ†æ
        system_metrics = metrics.get('system', {})
        if system_metrics:
            analysis['metrics']['system'] = {
                'cpu_usage': system_metrics.get('cpu_usage_percent', 0),
                'memory_usage': system_metrics.get('memory_usage_percent', 0),
                'disk_io': system_metrics.get('disk_io_percent', 0),
                'network_io': system_metrics.get('network_io_mbps', 0)
            }
        
        # ä¸åŸºçº¿æ¯”è¾ƒ
        if baseline:
            analysis.update(self._compare_with_baseline(analysis['metrics'], baseline.get('metrics', {})))
        
        # ç”Ÿæˆå»ºè®®
        analysis['recommendations'] = self._generate_backend_recommendations(analysis)
        
        return analysis
    
    def analyze_frontend_performance(self, data: Dict[str, Any], baseline: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ†æå‰ç«¯æ€§èƒ½æ•°æ®"""
        analysis = {
            'app_name': data.get('app_name', 'unknown'),
            'test_type': 'frontend',
            'timestamp': data.get('timestamp', ''),
            'environment': data.get('environment', 'unknown'),
            'metrics': {},
            'regression_detected': False,
            'improvements': [],
            'regressions': [],
            'recommendations': []
        }
        
        # Lighthouse æŒ‡æ ‡åˆ†æ
        lighthouse = data.get('lighthouse', {})
        if lighthouse:
            analysis['metrics']['lighthouse'] = {
                'performance': lighthouse.get('performance', 0),
                'accessibility': lighthouse.get('accessibility', 0),
                'best_practices': lighthouse.get('best_practices', 0),
                'seo': lighthouse.get('seo', 0)
            }
        
        # Core Web Vitals åˆ†æ
        core_vitals = data.get('core_web_vitals', {})
        if core_vitals:
            analysis['metrics']['core_web_vitals'] = {
                'lcp': core_vitals.get('largest_contentful_paint', 0),
                'fid': core_vitals.get('first_input_delay', 0),
                'cls': core_vitals.get('cumulative_layout_shift', 0),
                'fcp': core_vitals.get('first_contentful_paint', 0),
                'ttfb': core_vitals.get('time_to_first_byte', 0)
            }
        
        # èµ„æºåŠ è½½åˆ†æ
        resources = data.get('resources', {})
        if resources:
            analysis['metrics']['resources'] = {
                'total_size': resources.get('total_size_kb', 0),
                'js_size': resources.get('javascript_size_kb', 0),
                'css_size': resources.get('css_size_kb', 0),
                'image_size': resources.get('image_size_kb', 0),
                'font_size': resources.get('font_size_kb', 0),
                'load_time': resources.get('load_time_ms', 0)
            }
        
        # ä¸åŸºçº¿æ¯”è¾ƒ
        if baseline:
            analysis.update(self._compare_with_baseline(analysis['metrics'], baseline.get('metrics', {})))
        
        # ç”Ÿæˆå»ºè®®
        analysis['recommendations'] = self._generate_frontend_recommendations(analysis)
        
        return analysis
    
    def analyze_e2e_performance(self, data: Dict[str, Any], baseline: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ†æ E2E æ€§èƒ½æ•°æ®"""
        analysis = {
            'test_suite': data.get('test_suite', 'unknown'),
            'test_type': 'e2e',
            'timestamp': data.get('timestamp', ''),
            'environment': data.get('environment', 'unknown'),
            'metrics': {},
            'regression_detected': False,
            'improvements': [],
            'regressions': [],
            'recommendations': []
        }
        
        # åœºæ™¯æ‰§è¡Œæ—¶é—´åˆ†æ
        scenarios = data.get('scenarios', {})
        if scenarios:
            scenario_metrics = {}
            for scenario_name, scenario_data in scenarios.items():
                scenario_metrics[scenario_name] = {
                    'total_duration': scenario_data.get('total_duration_ms', 0),
                    'steps': scenario_data.get('steps', {}),
                    'success_rate': scenario_data.get('success_rate', 0),
                    'error_count': scenario_data.get('error_count', 0)
                }
            
            analysis['metrics']['scenarios'] = scenario_metrics
        
        # æ•´ä½“ç»Ÿè®¡
        summary = data.get('summary', {})
        if summary:
            analysis['metrics']['summary'] = {
                'total_scenarios': summary.get('total_scenarios', 0),
                'passed_scenarios': summary.get('passed_scenarios', 0),
                'failed_scenarios': summary.get('failed_scenarios', 0),
                'avg_duration': summary.get('avg_duration_ms', 0),
                'total_duration': summary.get('total_duration_ms', 0)
            }
        
        # ä¸åŸºçº¿æ¯”è¾ƒ
        if baseline:
            analysis.update(self._compare_with_baseline(analysis['metrics'], baseline.get('metrics', {})))
        
        # ç”Ÿæˆå»ºè®®
        analysis['recommendations'] = self._generate_e2e_recommendations(analysis)
        
        return analysis
    
    def _compare_with_baseline(self, current_metrics: Dict[str, Any], baseline_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸åŸºçº¿æ•°æ®æ¯”è¾ƒ"""
        comparison = {
            'regression_detected': False,
            'improvements': [],
            'regressions': [],
            'changes': {}
        }
        
        # å®šä¹‰å›å½’é˜ˆå€¼
        regression_thresholds = {
            'response_time': {'p95': 20, 'avg': 15},  # å“åº”æ—¶é—´å¢åŠ è¶…è¿‡ 20%/15% è§†ä¸ºå›å½’
            'throughput': {'rps': -10},  # ååé‡ä¸‹é™è¶…è¿‡ 10% è§†ä¸ºå›å½’
            'error_rate': {'percentage': 5},  # é”™è¯¯ç‡å¢åŠ è¶…è¿‡ 5% è§†ä¸ºå›å½’
            'lighthouse': {'performance': -5},  # Lighthouse æ€§èƒ½åˆ†æ•°ä¸‹é™è¶…è¿‡ 5 åˆ†è§†ä¸ºå›å½’
            'core_web_vitals': {'lcp': 20, 'fid': 50, 'cls': 10}  # Core Web Vitals æ¶åŒ–é˜ˆå€¼
        }
        
        def compare_metric(current_value: float, baseline_value: float, threshold: float, higher_is_better: bool = False) -> Tuple[str, float]:
            """æ¯”è¾ƒå•ä¸ªæŒ‡æ ‡"""
            if baseline_value == 0:
                return 'no_baseline', 0
            
            change_percent = ((current_value - baseline_value) / baseline_value) * 100
            
            if higher_is_better:
                if change_percent < -threshold:
                    return 'regression', change_percent
                elif change_percent > threshold:
                    return 'improvement', change_percent
            else:
                if change_percent > threshold:
                    return 'regression', change_percent
                elif change_percent < -threshold:
                    return 'improvement', change_percent
            
            return 'stable', change_percent
        
        # æ¯”è¾ƒå„ç±»æŒ‡æ ‡
        for metric_category, current_data in current_metrics.items():
            if metric_category not in baseline_metrics:
                continue
            
            baseline_data = baseline_metrics[metric_category]
            category_changes = {}
            
            if metric_category == 'response_time':
                for key, current_val in current_data.items():
                    if key in baseline_data and key in regression_thresholds['response_time']:
                        threshold = regression_thresholds['response_time'][key]
                        status, change = compare_metric(current_val, baseline_data[key], threshold)
                        category_changes[key] = {'status': status, 'change_percent': change, 'current': current_val, 'baseline': baseline_data[key]}
                        
                        if status == 'regression':
                            comparison['regressions'].append(f"å“åº”æ—¶é—´ {key} å¢åŠ äº† {change:.1f}% ({baseline_data[key]:.0f}ms -> {current_val:.0f}ms)")
                            comparison['regression_detected'] = True
                        elif status == 'improvement':
                            comparison['improvements'].append(f"å“åº”æ—¶é—´ {key} æ”¹å–„äº† {abs(change):.1f}% ({baseline_data[key]:.0f}ms -> {current_val:.0f}ms)")
            
            elif metric_category == 'throughput':
                if 'rps' in current_data and 'rps' in baseline_data:
                    threshold = abs(regression_thresholds['throughput']['rps'])
                    status, change = compare_metric(current_data['rps'], baseline_data['rps'], threshold, higher_is_better=True)
                    category_changes['rps'] = {'status': status, 'change_percent': change, 'current': current_data['rps'], 'baseline': baseline_data['rps']}
                    
                    if status == 'regression':
                        comparison['regressions'].append(f"ååé‡ä¸‹é™äº† {abs(change):.1f}% ({baseline_data['rps']:.0f} -> {current_data['rps']:.0f} RPS)")
                        comparison['regression_detected'] = True
                    elif status == 'improvement':
                        comparison['improvements'].append(f"ååé‡æå‡äº† {change:.1f}% ({baseline_data['rps']:.0f} -> {current_data['rps']:.0f} RPS)")
            
            elif metric_category == 'error_rate':
                if 'percentage' in current_data and 'percentage' in baseline_data:
                    threshold = regression_thresholds['error_rate']['percentage']
                    current_val = current_data['percentage']
                    baseline_val = baseline_data['percentage']
                    change = current_val - baseline_val
                    
                    category_changes['percentage'] = {'status': 'stable', 'change_absolute': change, 'current': current_val, 'baseline': baseline_val}
                    
                    if change > threshold:
                        category_changes['percentage']['status'] = 'regression'
                        comparison['regressions'].append(f"é”™è¯¯ç‡å¢åŠ äº† {change:.1f}% ({baseline_val:.1f}% -> {current_val:.1f}%)")
                        comparison['regression_detected'] = True
                    elif change < -threshold:
                        category_changes['percentage']['status'] = 'improvement'
                        comparison['improvements'].append(f"é”™è¯¯ç‡é™ä½äº† {abs(change):.1f}% ({baseline_val:.1f}% -> {current_val:.1f}%)")
            
            elif metric_category == 'lighthouse':
                for key, current_val in current_data.items():
                    if key in baseline_data:
                        threshold = abs(regression_thresholds['lighthouse']['performance'])
                        status, change = compare_metric(current_val, baseline_data[key], threshold, higher_is_better=True)
                        category_changes[key] = {'status': status, 'change_percent': change, 'current': current_val, 'baseline': baseline_data[key]}
                        
                        if status == 'regression':
                            comparison['regressions'].append(f"Lighthouse {key} åˆ†æ•°ä¸‹é™äº† {abs(change):.1f}% ({baseline_data[key]:.0f} -> {current_val:.0f})")
                            comparison['regression_detected'] = True
                        elif status == 'improvement':
                            comparison['improvements'].append(f"Lighthouse {key} åˆ†æ•°æå‡äº† {change:.1f}% ({baseline_data[key]:.0f} -> {current_val:.0f})")
            
            comparison['changes'][metric_category] = category_changes
        
        return comparison
    
    def _generate_backend_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆåç«¯æ€§èƒ½å»ºè®®"""
        recommendations = []
        metrics = analysis.get('metrics', {})
        
        # å“åº”æ—¶é—´å»ºè®®
        response_time = metrics.get('response_time', {})
        if response_time.get('p95', 0) > 1000:  # P95 è¶…è¿‡ 1 ç§’
            recommendations.append("P95 å“åº”æ—¶é—´è¶…è¿‡ 1 ç§’ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œç¼“å­˜ç­–ç•¥")
        
        if response_time.get('avg', 0) > 500:  # å¹³å‡å“åº”æ—¶é—´è¶…è¿‡ 500ms
            recommendations.append("å¹³å‡å“åº”æ—¶é—´è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥æ…¢æŸ¥è¯¢å’Œ API æ€§èƒ½ç“¶é¢ˆ")
        
        # ååé‡å»ºè®®
        throughput = metrics.get('throughput', {})
        if throughput.get('rps', 0) < 100:  # RPS ä½äº 100
            recommendations.append("ååé‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–åº”ç”¨æ€§èƒ½å’Œå¢åŠ å¹¶å‘å¤„ç†èƒ½åŠ›")
        
        # é”™è¯¯ç‡å»ºè®®
        error_rate = metrics.get('error_rate', {})
        if error_rate.get('percentage', 0) > 1:  # é”™è¯¯ç‡è¶…è¿‡ 1%
            recommendations.append("é”™è¯¯ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥é”™è¯¯æ—¥å¿—å’Œå¼‚å¸¸å¤„ç†")
        
        # ç³»ç»Ÿèµ„æºå»ºè®®
        system = metrics.get('system', {})
        if system.get('cpu_usage', 0) > 80:
            recommendations.append("CPU ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•æˆ–å¢åŠ è®¡ç®—èµ„æº")
        
        if system.get('memory_usage', 0) > 85:
            recommendations.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼å’Œä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        
        # å›å½’å»ºè®®
        if analysis.get('regression_detected'):
            recommendations.append("æ£€æµ‹åˆ°æ€§èƒ½å›å½’ï¼Œå»ºè®®å›æ»šæœ€è¿‘çš„ä»£ç å˜æ›´æˆ–è¿›è¡Œæ€§èƒ½ä¼˜åŒ–")
        
        return recommendations
    
    def _generate_frontend_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå‰ç«¯æ€§èƒ½å»ºè®®"""
        recommendations = []
        metrics = analysis.get('metrics', {})
        
        # Lighthouse å»ºè®®
        lighthouse = metrics.get('lighthouse', {})
        if lighthouse.get('performance', 0) < 90:
            recommendations.append("Lighthouse æ€§èƒ½åˆ†æ•°è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–èµ„æºåŠ è½½å’Œæ¸²æŸ“æ€§èƒ½")
        
        if lighthouse.get('accessibility', 0) < 90:
            recommendations.append("å¯è®¿é—®æ€§åˆ†æ•°è¾ƒä½ï¼Œå»ºè®®æ”¹å–„æ— éšœç¢è®¾è®¡")
        
        # Core Web Vitals å»ºè®®
        vitals = metrics.get('core_web_vitals', {})
        if vitals.get('lcp', 0) > 2500:  # LCP è¶…è¿‡ 2.5 ç§’
            recommendations.append("LCP è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–å…³é”®èµ„æºåŠ è½½å’ŒæœåŠ¡å™¨å“åº”æ—¶é—´")
        
        if vitals.get('fid', 0) > 100:  # FID è¶…è¿‡ 100ms
            recommendations.append("FID è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ– JavaScript æ‰§è¡Œå’Œå‡å°‘ä¸»çº¿ç¨‹é˜»å¡")
        
        if vitals.get('cls', 0) > 0.1:  # CLS è¶…è¿‡ 0.1
            recommendations.append("CLS è¿‡é«˜ï¼Œå»ºè®®ä¸ºå›¾ç‰‡å’Œå¹¿å‘Šé¢„ç•™ç©ºé—´ï¼Œé¿å…å¸ƒå±€åç§»")
        
        # èµ„æºå¤§å°å»ºè®®
        resources = metrics.get('resources', {})
        if resources.get('total_size', 0) > 3000:  # æ€»å¤§å°è¶…è¿‡ 3MB
            recommendations.append("èµ„æºæ€»å¤§å°è¿‡å¤§ï¼Œå»ºè®®å‹ç¼©å›¾ç‰‡ã€ä»£ç åˆ†å‰²å’Œå¯ç”¨ gzip")
        
        if resources.get('js_size', 0) > 1000:  # JS å¤§å°è¶…è¿‡ 1MB
            recommendations.append("JavaScript æ–‡ä»¶è¿‡å¤§ï¼Œå»ºè®®è¿›è¡Œä»£ç åˆ†å‰²å’Œ Tree Shaking")
        
        # å›å½’å»ºè®®
        if analysis.get('regression_detected'):
            recommendations.append("æ£€æµ‹åˆ°å‰ç«¯æ€§èƒ½å›å½’ï¼Œå»ºè®®æ£€æŸ¥æœ€è¿‘çš„ä»£ç å˜æ›´å’Œèµ„æºä¼˜åŒ–")
        
        return recommendations
    
    def _generate_e2e_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆ E2E æ€§èƒ½å»ºè®®"""
        recommendations = []
        metrics = analysis.get('metrics', {})
        
        # åœºæ™¯æ‰§è¡Œæ—¶é—´å»ºè®®
        scenarios = metrics.get('scenarios', {})
        for scenario_name, scenario_data in scenarios.items():
            duration = scenario_data.get('total_duration', 0)
            if duration > 30000:  # è¶…è¿‡ 30 ç§’
                recommendations.append(f"åœºæ™¯ '{scenario_name}' æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({duration/1000:.1f}s)ï¼Œå»ºè®®ä¼˜åŒ–æµ‹è¯•æ­¥éª¤")
            
            success_rate = scenario_data.get('success_rate', 0)
            if success_rate < 95:  # æˆåŠŸç‡ä½äº 95%
                recommendations.append(f"åœºæ™¯ '{scenario_name}' æˆåŠŸç‡è¾ƒä½ ({success_rate:.1f}%)ï¼Œå»ºè®®æ£€æŸ¥æµ‹è¯•ç¨³å®šæ€§")
        
        # æ•´ä½“ç»Ÿè®¡å»ºè®®
        summary = metrics.get('summary', {})
        if summary.get('failed_scenarios', 0) > 0:
            failed_count = summary.get('failed_scenarios', 0)
            total_count = summary.get('total_scenarios', 1)
            failure_rate = (failed_count / total_count) * 100
            recommendations.append(f"æœ‰ {failed_count} ä¸ªåœºæ™¯å¤±è´¥ ({failure_rate:.1f}%)ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥åŸå› ")
        
        avg_duration = summary.get('avg_duration', 0)
        if avg_duration > 15000:  # å¹³å‡æ‰§è¡Œæ—¶é—´è¶…è¿‡ 15 ç§’
            recommendations.append(f"å¹³å‡åœºæ™¯æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({avg_duration/1000:.1f}s)ï¼Œå»ºè®®ä¼˜åŒ–æµ‹è¯•æ•ˆç‡")
        
        # å›å½’å»ºè®®
        if analysis.get('regression_detected'):
            recommendations.append("æ£€æµ‹åˆ° E2E æ€§èƒ½å›å½’ï¼Œå»ºè®®æ£€æŸ¥åº”ç”¨æ€§èƒ½å’Œæµ‹è¯•ç¯å¢ƒ")
        
        return recommendations
    
    def generate_performance_charts(self, analyses: List[Dict[str, Any]], output_dir: str) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        chart_files = []
        
        # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç»„
        backend_analyses = [a for a in analyses if a.get('test_type') == 'backend']
        frontend_analyses = [a for a in analyses if a.get('test_type') == 'frontend']
        e2e_analyses = [a for a in analyses if a.get('test_type') == 'e2e']
        
        # ç”Ÿæˆåç«¯æ€§èƒ½å›¾è¡¨
        if backend_analyses:
            chart_file = self._generate_backend_charts(backend_analyses, output_path)
            if chart_file:
                chart_files.append(chart_file)
        
        # ç”Ÿæˆå‰ç«¯æ€§èƒ½å›¾è¡¨
        if frontend_analyses:
            chart_file = self._generate_frontend_charts(frontend_analyses, output_path)
            if chart_file:
                chart_files.append(chart_file)
        
        # ç”Ÿæˆ E2E æ€§èƒ½å›¾è¡¨
        if e2e_analyses:
            chart_file = self._generate_e2e_charts(e2e_analyses, output_path)
            if chart_file:
                chart_files.append(chart_file)
        
        return chart_files
    
    def _generate_backend_charts(self, analyses: List[Dict[str, Any]], output_path: Path) -> Optional[str]:
        """ç”Ÿæˆåç«¯æ€§èƒ½å›¾è¡¨"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('åç«¯æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')
            
            # æå–æ•°æ®
            services = []
            response_times = []
            throughputs = []
            error_rates = []
            cpu_usages = []
            
            for analysis in analyses:
                service_name = analysis.get('service_name', 'Unknown')
                metrics = analysis.get('metrics', {})
                
                services.append(service_name)
                response_times.append(metrics.get('response_time', {}).get('p95', 0))
                throughputs.append(metrics.get('throughput', {}).get('rps', 0))
                error_rates.append(metrics.get('error_rate', {}).get('percentage', 0))
                cpu_usages.append(metrics.get('system', {}).get('cpu_usage', 0))
            
            # å“åº”æ—¶é—´å›¾è¡¨
            axes[0, 0].bar(services, response_times, color='skyblue')
            axes[0, 0].set_title('P95 å“åº”æ—¶é—´ (ms)')
            axes[0, 0].set_ylabel('å“åº”æ—¶é—´ (ms)')
            axes[0, 0].tick_params(axis='x', rotation=45)
            
            # ååé‡å›¾è¡¨
            axes[0, 1].bar(services, throughputs, color='lightgreen')
            axes[0, 1].set_title('ååé‡ (RPS)')
            axes[0, 1].set_ylabel('è¯·æ±‚/ç§’')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            # é”™è¯¯ç‡å›¾è¡¨
            axes[1, 0].bar(services, error_rates, color='lightcoral')
            axes[1, 0].set_title('é”™è¯¯ç‡ (%)')
            axes[1, 0].set_ylabel('é”™è¯¯ç‡ (%)')
            axes[1, 0].tick_params(axis='x', rotation=45)
            
            # CPU ä½¿ç”¨ç‡å›¾è¡¨
            axes[1, 1].bar(services, cpu_usages, color='orange')
            axes[1, 1].set_title('CPU ä½¿ç”¨ç‡ (%)')
            axes[1, 1].set_ylabel('CPU ä½¿ç”¨ç‡ (%)')
            axes[1, 1].tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            
            chart_file = output_path / 'backend-performance.png'
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            return str(chart_file)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆåç«¯æ€§èƒ½å›¾è¡¨å¤±è´¥: {e}")
            return None
    
    def _generate_frontend_charts(self, analyses: List[Dict[str, Any]], output_path: Path) -> Optional[str]:
        """ç”Ÿæˆå‰ç«¯æ€§èƒ½å›¾è¡¨"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('å‰ç«¯æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')
            
            # æå–æ•°æ®
            apps = []
            lighthouse_scores = []
            lcp_values = []
            fid_values = []
            cls_values = []
            
            for analysis in analyses:
                app_name = analysis.get('app_name', 'Unknown')
                metrics = analysis.get('metrics', {})
                
                apps.append(app_name)
                lighthouse = metrics.get('lighthouse', {})
                lighthouse_scores.append([
                    lighthouse.get('performance', 0),
                    lighthouse.get('accessibility', 0),
                    lighthouse.get('best_practices', 0),
                    lighthouse.get('seo', 0)
                ])
                
                vitals = metrics.get('core_web_vitals', {})
                lcp_values.append(vitals.get('lcp', 0))
                fid_values.append(vitals.get('fid', 0))
                cls_values.append(vitals.get('cls', 0))
            
            # Lighthouse åˆ†æ•°é›·è¾¾å›¾
            if lighthouse_scores:
                categories = ['Performance', 'Accessibility', 'Best Practices', 'SEO']
                
                # è®¡ç®—å¹³å‡åˆ†æ•°
                avg_scores = [statistics.mean([scores[i] for scores in lighthouse_scores]) for i in range(4)]
                
                angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
                avg_scores += avg_scores[:1]  # é—­åˆå›¾å½¢
                angles += angles[:1]
                
                ax = plt.subplot(2, 2, 1, projection='polar')
                ax.plot(angles, avg_scores, 'o-', linewidth=2, color='blue')
                ax.fill(angles, avg_scores, alpha=0.25, color='blue')
                ax.set_xticks(angles[:-1])
                ax.set_xticklabels(categories)
                ax.set_ylim(0, 100)
                ax.set_title('Lighthouse å¹³å‡åˆ†æ•°')
            
            # LCP å›¾è¡¨
            axes[0, 1].bar(apps, lcp_values, color='orange')
            axes[0, 1].set_title('Largest Contentful Paint (ms)')
            axes[0, 1].set_ylabel('LCP (ms)')
            axes[0, 1].tick_params(axis='x', rotation=45)
            axes[0, 1].axhline(y=2500, color='red', linestyle='--', alpha=0.7, label='Poor (>2.5s)')
            axes[0, 1].axhline(y=4000, color='orange', linestyle='--', alpha=0.7, label='Needs Improvement (>4s)')
            
            # FID å›¾è¡¨
            axes[1, 0].bar(apps, fid_values, color='green')
            axes[1, 0].set_title('First Input Delay (ms)')
            axes[1, 0].set_ylabel('FID (ms)')
            axes[1, 0].tick_params(axis='x', rotation=45)
            axes[1, 0].axhline(y=100, color='red', linestyle='--', alpha=0.7, label='Poor (>100ms)')
            axes[1, 0].axhline(y=300, color='orange', linestyle='--', alpha=0.7, label='Needs Improvement (>300ms)')
            
            # CLS å›¾è¡¨
            axes[1, 1].bar(apps, cls_values, color='purple')
            axes[1, 1].set_title('Cumulative Layout Shift')
            axes[1, 1].set_ylabel('CLS')
            axes[1, 1].tick_params(axis='x', rotation=45)
            axes[1, 1].axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Poor (>0.1)')
            axes[1, 1].axhline(y=0.25, color='orange', linestyle='--', alpha=0.7, label='Needs Improvement (>0.25)')
            
            plt.tight_layout()
            
            chart_file = output_path / 'frontend-performance.png'
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            return str(chart_file)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå‰ç«¯æ€§èƒ½å›¾è¡¨å¤±è´¥: {e}")
            return None
    
    def _generate_e2e_charts(self, analyses: List[Dict[str, Any]], output_path: Path) -> Optional[str]:
        """ç”Ÿæˆ E2E æ€§èƒ½å›¾è¡¨"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('E2E æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')
            
            # æå–æ•°æ®
            test_suites = []
            avg_durations = []
            success_rates = []
            scenario_counts = []
            
            all_scenarios = {}
            
            for analysis in analyses:
                suite_name = analysis.get('test_suite', 'Unknown')
                metrics = analysis.get('metrics', {})
                
                test_suites.append(suite_name)
                
                summary = metrics.get('summary', {})
                avg_durations.append(summary.get('avg_duration', 0) / 1000)  # è½¬æ¢ä¸ºç§’
                
                total_scenarios = summary.get('total_scenarios', 0)
                passed_scenarios = summary.get('passed_scenarios', 0)
                success_rate = (passed_scenarios / total_scenarios * 100) if total_scenarios > 0 else 0
                success_rates.append(success_rate)
                scenario_counts.append(total_scenarios)
                
                # æ”¶é›†æ‰€æœ‰åœºæ™¯æ•°æ®
                scenarios = metrics.get('scenarios', {})
                for scenario_name, scenario_data in scenarios.items():
                    if scenario_name not in all_scenarios:
                        all_scenarios[scenario_name] = []
                    all_scenarios[scenario_name].append(scenario_data.get('total_duration', 0) / 1000)
            
            # å¹³å‡æ‰§è¡Œæ—¶é—´å›¾è¡¨
            axes[0, 0].bar(test_suites, avg_durations, color='skyblue')
            axes[0, 0].set_title('å¹³å‡åœºæ™¯æ‰§è¡Œæ—¶é—´ (ç§’)')
            axes[0, 0].set_ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
            axes[0, 0].tick_params(axis='x', rotation=45)
            
            # æˆåŠŸç‡å›¾è¡¨
            colors = ['green' if rate >= 95 else 'orange' if rate >= 90 else 'red' for rate in success_rates]
            axes[0, 1].bar(test_suites, success_rates, color=colors)
            axes[0, 1].set_title('æµ‹è¯•æˆåŠŸç‡ (%)')
            axes[0, 1].set_ylabel('æˆåŠŸç‡ (%)')
            axes[0, 1].set_ylim(0, 100)
            axes[0, 1].tick_params(axis='x', rotation=45)
            axes[0, 1].axhline(y=95, color='green', linestyle='--', alpha=0.7, label='ç›®æ ‡ (95%)')
            
            # åœºæ™¯æ•°é‡å›¾è¡¨
            axes[1, 0].bar(test_suites, scenario_counts, color='lightgreen')
            axes[1, 0].set_title('æµ‹è¯•åœºæ™¯æ•°é‡')
            axes[1, 0].set_ylabel('åœºæ™¯æ•°é‡')
            axes[1, 0].tick_params(axis='x', rotation=45)
            
            # å„åœºæ™¯æ‰§è¡Œæ—¶é—´å¯¹æ¯”
            if all_scenarios:
                scenario_names = list(all_scenarios.keys())[:10]  # åªæ˜¾ç¤ºå‰ 10 ä¸ªåœºæ™¯
                scenario_avg_times = [statistics.mean(all_scenarios[name]) for name in scenario_names]
                
                axes[1, 1].barh(scenario_names, scenario_avg_times, color='orange')
                axes[1, 1].set_title('å„åœºæ™¯å¹³å‡æ‰§è¡Œæ—¶é—´ (ç§’)')
                axes[1, 1].set_xlabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
            
            plt.tight_layout()
            
            chart_file = output_path / 'e2e-performance.png'
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            return str(chart_file)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆ E2E æ€§èƒ½å›¾è¡¨å¤±è´¥: {e}")
            return None
    
    def generate_performance_report(self, analyses: List[Dict[str, Any]], output_file: str, chart_files: List[str] = None) -> None:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        # æŠ¥å‘Šæ¨¡æ¿
        report_template = Template("""
# PhoenixCoder æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´:** {{ generation_time }}
**æµ‹è¯•ç¯å¢ƒ:** {{ environment }}
**æµ‹è¯•å‘¨æœŸ:** {{ test_period }}

## ğŸ“Š æ€»ä½“æ¦‚è§ˆ

{% if summary %}
- **æ€»æµ‹è¯•æ•°:** {{ summary.total_tests }}
- **å›å½’æ£€æµ‹:** {{ summary.regressions_detected }} ä¸ª
- **æ€§èƒ½æ”¹è¿›:** {{ summary.improvements_detected }} ä¸ª
- **æ•´ä½“çŠ¶æ€:** {{ summary.overall_status }}
{% endif %}

## ğŸš€ åç«¯æœåŠ¡æ€§èƒ½

{% for analysis in backend_analyses %}
### {{ analysis.service_name }}

**æµ‹è¯•æ—¶é—´:** {{ analysis.timestamp }}

#### å…³é”®æŒ‡æ ‡
{% if analysis.metrics.response_time %}
- **å“åº”æ—¶é—´:**
  - å¹³å‡: {{ "%.0f" | format(analysis.metrics.response_time.avg) }}ms
  - P95: {{ "%.0f" | format(analysis.metrics.response_time.p95) }}ms
  - P99: {{ "%.0f" | format(analysis.metrics.response_time.p99) }}ms
{% endif %}

{% if analysis.metrics.throughput %}
- **ååé‡:** {{ "%.0f" | format(analysis.metrics.throughput.rps) }} RPS
{% endif %}

{% if analysis.metrics.error_rate %}
- **é”™è¯¯ç‡:** {{ "%.2f" | format(analysis.metrics.error_rate.percentage) }}%
{% endif %}

{% if analysis.metrics.system %}
- **ç³»ç»Ÿèµ„æº:**
  - CPU: {{ "%.1f" | format(analysis.metrics.system.cpu_usage) }}%
  - å†…å­˜: {{ "%.1f" | format(analysis.metrics.system.memory_usage) }}%
{% endif %}

#### æ€§èƒ½å˜åŒ–
{% if analysis.improvements %}
**âœ… æ”¹è¿›:**
{% for improvement in analysis.improvements %}
- {{ improvement }}
{% endfor %}
{% endif %}

{% if analysis.regressions %}
**âš ï¸ å›å½’:**
{% for regression in analysis.regressions %}
- {{ regression }}
{% endfor %}
{% endif %}

#### å»ºè®®
{% for recommendation in analysis.recommendations %}
- {{ recommendation }}
{% endfor %}

---
{% endfor %}

## ğŸŒ å‰ç«¯åº”ç”¨æ€§èƒ½

{% for analysis in frontend_analyses %}
### {{ analysis.app_name }}

**æµ‹è¯•æ—¶é—´:** {{ analysis.timestamp }}

#### Lighthouse åˆ†æ•°
{% if analysis.metrics.lighthouse %}
- **æ€§èƒ½:** {{ analysis.metrics.lighthouse.performance }}/100
- **å¯è®¿é—®æ€§:** {{ analysis.metrics.lighthouse.accessibility }}/100
- **æœ€ä½³å®è·µ:** {{ analysis.metrics.lighthouse.best_practices }}/100
- **SEO:** {{ analysis.metrics.lighthouse.seo }}/100
{% endif %}

#### Core Web Vitals
{% if analysis.metrics.core_web_vitals %}
- **LCP:** {{ "%.0f" | format(analysis.metrics.core_web_vitals.lcp) }}ms
- **FID:** {{ "%.0f" | format(analysis.metrics.core_web_vitals.fid) }}ms
- **CLS:** {{ "%.3f" | format(analysis.metrics.core_web_vitals.cls) }}
{% endif %}

#### èµ„æºå¤§å°
{% if analysis.metrics.resources %}
- **æ€»å¤§å°:** {{ "%.0f" | format(analysis.metrics.resources.total_size) }}KB
- **JavaScript:** {{ "%.0f" | format(analysis.metrics.resources.js_size) }}KB
- **CSS:** {{ "%.0f" | format(analysis.metrics.resources.css_size) }}KB
- **å›¾ç‰‡:** {{ "%.0f" | format(analysis.metrics.resources.image_size) }}KB
{% endif %}

#### æ€§èƒ½å˜åŒ–
{% if analysis.improvements %}
**âœ… æ”¹è¿›:**
{% for improvement in analysis.improvements %}
- {{ improvement }}
{% endfor %}
{% endif %}

{% if analysis.regressions %}
**âš ï¸ å›å½’:**
{% for regression in analysis.regressions %}
- {{ regression }}
{% endfor %}
{% endif %}

#### å»ºè®®
{% for recommendation in analysis.recommendations %}
- {{ recommendation }}
{% endfor %}

---
{% endfor %}

## ğŸ”„ E2E æµ‹è¯•æ€§èƒ½

{% for analysis in e2e_analyses %}
### {{ analysis.test_suite }}

**æµ‹è¯•æ—¶é—´:** {{ analysis.timestamp }}

#### æ‰§è¡Œç»Ÿè®¡
{% if analysis.metrics.summary %}
- **æ€»åœºæ™¯æ•°:** {{ analysis.metrics.summary.total_scenarios }}
- **é€šè¿‡åœºæ™¯:** {{ analysis.metrics.summary.passed_scenarios }}
- **å¤±è´¥åœºæ™¯:** {{ analysis.metrics.summary.failed_scenarios }}
- **å¹³å‡æ‰§è¡Œæ—¶é—´:** {{ "%.1f" | format(analysis.metrics.summary.avg_duration / 1000) }}ç§’
- **æ€»æ‰§è¡Œæ—¶é—´:** {{ "%.1f" | format(analysis.metrics.summary.total_duration / 1000) }}ç§’
{% endif %}

#### åœºæ™¯è¯¦æƒ…
{% if analysis.metrics.scenarios %}
{% for scenario_name, scenario_data in analysis.metrics.scenarios.items() %}
- **{{ scenario_name }}:** {{ "%.1f" | format(scenario_data.total_duration / 1000) }}ç§’ (æˆåŠŸç‡: {{ "%.1f" | format(scenario_data.success_rate) }}%)
{% endfor %}
{% endif %}

#### æ€§èƒ½å˜åŒ–
{% if analysis.improvements %}
**âœ… æ”¹è¿›:**
{% for improvement in analysis.improvements %}
- {{ improvement }}
{% endfor %}
{% endif %}

{% if analysis.regressions %}
**âš ï¸ å›å½’:**
{% for regression in analysis.regressions %}
- {{ regression }}
{% endfor %}
{% endif %}

#### å»ºè®®
{% for recommendation in analysis.recommendations %}
- {{ recommendation }}
{% endfor %}

---
{% endfor %}

## ğŸ“ˆ æ€§èƒ½å›¾è¡¨

{% if chart_files %}
{% for chart_file in chart_files %}
![æ€§èƒ½å›¾è¡¨]({{ chart_file }})
{% endfor %}
{% endif %}

## ğŸ¯ æ€»ä½“å»ºè®®

{% if overall_recommendations %}
{% for recommendation in overall_recommendations %}
- {{ recommendation }}
{% endfor %}
{% endif %}

---

*æŠ¥å‘Šç”± PhoenixCoder æ€§èƒ½åˆ†æå™¨è‡ªåŠ¨ç”Ÿæˆ*
        """)
        
        # åˆ†ç±»åˆ†æç»“æœ
        backend_analyses = [a for a in analyses if a.get('test_type') == 'backend']
        frontend_analyses = [a for a in analyses if a.get('test_type') == 'frontend']
        e2e_analyses = [a for a in analyses if a.get('test_type') == 'e2e']
        
        # ç”Ÿæˆæ€»ä½“ç»Ÿè®¡
        total_tests = len(analyses)
        regressions_detected = sum(1 for a in analyses if a.get('regression_detected'))
        improvements_detected = sum(1 for a in analyses if a.get('improvements'))
        
        overall_status = "è‰¯å¥½"
        if regressions_detected > 0:
            overall_status = "éœ€è¦å…³æ³¨"
        elif improvements_detected > 0:
            overall_status = "æŒç»­æ”¹è¿›"
        
        summary = {
            'total_tests': total_tests,
            'regressions_detected': regressions_detected,
            'improvements_detected': improvements_detected,
            'overall_status': overall_status
        }
        
        # ç”Ÿæˆæ€»ä½“å»ºè®®
        overall_recommendations = []
        if regressions_detected > 0:
            overall_recommendations.append(f"æ£€æµ‹åˆ° {regressions_detected} ä¸ªæ€§èƒ½å›å½’ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†")
        
        if improvements_detected > 0:
            overall_recommendations.append(f"å‘ç° {improvements_detected} ä¸ªæ€§èƒ½æ”¹è¿›ï¼Œç»§ç»­ä¿æŒ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é«˜é£é™©é—®é¢˜
        high_risk_issues = []
        for analysis in analyses:
            if analysis.get('test_type') == 'backend':
                metrics = analysis.get('metrics', {})
                if metrics.get('error_rate', {}).get('percentage', 0) > 5:
                    high_risk_issues.append(f"{analysis.get('service_name')} é”™è¯¯ç‡è¿‡é«˜")
            elif analysis.get('test_type') == 'frontend':
                metrics = analysis.get('metrics', {})
                if metrics.get('lighthouse', {}).get('performance', 100) < 70:
                    high_risk_issues.append(f"{analysis.get('app_name')} Lighthouse æ€§èƒ½åˆ†æ•°è¿‡ä½")
        
        if high_risk_issues:
            overall_recommendations.extend([f"é«˜é£é™©é—®é¢˜: {issue}" for issue in high_risk_issues])
        
        # æ¸²æŸ“æŠ¥å‘Š
        report_content = report_template.render(
            generation_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            environment='æµ‹è¯•ç¯å¢ƒ',
            test_period='å½“å‰æµ‹è¯•å‘¨æœŸ',
            summary=summary,
            backend_analyses=backend_analyses,
            frontend_analyses=frontend_analyses,
            e2e_analyses=e2e_analyses,
            chart_files=chart_files or [],
            overall_recommendations=overall_recommendations
        )
        
        # ä¿å­˜æŠ¥å‘Š
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
    
    def analyze_all_results(self) -> List[Dict[str, Any]]:
        """åˆ†ææ‰€æœ‰æ€§èƒ½æµ‹è¯•ç»“æœ"""
        analyses = []
        
        if not self.results_dir.exists():
            logger.warning(f"ç»“æœç›®å½•ä¸å­˜åœ¨: {self.results_dir}")
            return analyses
        
        # æŸ¥æ‰¾æ‰€æœ‰æ€§èƒ½æµ‹è¯•ç»“æœæ–‡ä»¶
        result_files = list(self.results_dir.glob('**/*.json'))
        
        for result_file in result_files:
            logger.info(f"åˆ†ææ–‡ä»¶: {result_file}")
            
            data = self.load_performance_data(result_file)
            if not data:
                continue
            
            # æ ¹æ®æ–‡ä»¶åæˆ–æ•°æ®å†…å®¹åˆ¤æ–­æµ‹è¯•ç±»å‹
            test_type = self._determine_test_type(result_file, data)
            
            if test_type == 'backend':
                # åŠ è½½åŸºçº¿æ•°æ®
                service_name = data.get('service_name', result_file.stem)
                baseline = self.load_baseline_data(service_name, 'backend')
                analysis = self.analyze_backend_performance(data, baseline)
            elif test_type == 'frontend':
                app_name = data.get('app_name', result_file.stem)
                baseline = self.load_baseline_data(app_name, 'frontend')
                analysis = self.analyze_frontend_performance(data, baseline)
            elif test_type == 'e2e':
                test_suite = data.get('test_suite', result_file.stem)
                baseline = self.load_baseline_data(test_suite, 'e2e')
                analysis = self.analyze_e2e_performance(data, baseline)
            else:
                logger.warning(f"æ— æ³•ç¡®å®šæµ‹è¯•ç±»å‹: {result_file}")
                continue
            
            analyses.append(analysis)
        
        return analyses
    
    def _determine_test_type(self, file_path: Path, data: Dict[str, Any]) -> str:
        """ç¡®å®šæµ‹è¯•ç±»å‹"""
        # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­
        file_name = file_path.name.lower()
        if 'backend' in file_name or 'server' in file_name or 'api' in file_name:
            return 'backend'
        elif 'frontend' in file_name or 'lighthouse' in file_name or 'web' in file_name:
            return 'frontend'
        elif 'e2e' in file_name or 'playwright' in file_name:
            return 'e2e'
        
        # æ ¹æ®æ•°æ®å†…å®¹åˆ¤æ–­
        if 'service_name' in data or 'response_times' in data.get('metrics', {}):
            return 'backend'
        elif 'app_name' in data or 'lighthouse' in data:
            return 'frontend'
        elif 'test_suite' in data or 'scenarios' in data.get('metrics', {}):
            return 'e2e'
        
        return 'unknown'

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='PhoenixCoder æ€§èƒ½åˆ†æå™¨')
    parser.add_argument('--results-dir', required=True, help='æ€§èƒ½æµ‹è¯•ç»“æœç›®å½•')
    parser.add_argument('--baseline-dir', help='æ€§èƒ½åŸºçº¿æ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', default='performance-analysis', help='åˆ†æç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--report-file', default='performance-report.md', help='æ€§èƒ½æŠ¥å‘Šæ–‡ä»¶å')
    parser.add_argument('--generate-charts', action='store_true', help='ç”Ÿæˆæ€§èƒ½å›¾è¡¨')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
    analyzer = PerformanceAnalyzer(args.results_dir, args.baseline_dir)
    
    try:
        # åˆ†ææ‰€æœ‰ç»“æœ
        logger.info("å¼€å§‹åˆ†ææ€§èƒ½æµ‹è¯•ç»“æœ...")
        analyses = analyzer.analyze_all_results()
        
        if not analyses:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ€§èƒ½æµ‹è¯•ç»“æœ")
            return
        
        logger.info(f"åˆ†æäº† {len(analyses)} ä¸ªæ€§èƒ½æµ‹è¯•ç»“æœ")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = output_dir / 'analysis-results.json'
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analyses, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"åˆ†æç»“æœå·²ä¿å­˜: {analysis_file}")
        
        # ç”Ÿæˆå›¾è¡¨
        chart_files = []
        if args.generate_charts:
            logger.info("ç”Ÿæˆæ€§èƒ½å›¾è¡¨...")
            chart_files = analyzer.generate_performance_charts(analyses, str(output_dir))
            logger.info(f"ç”Ÿæˆäº† {len(chart_files)} ä¸ªå›¾è¡¨æ–‡ä»¶")
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = output_dir / args.report_file
        logger.info("ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
        analyzer.generate_performance_report(analyses, str(report_file), chart_files)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        regressions = sum(1 for a in analyses if a.get('regression_detected'))
        improvements = sum(1 for a in analyses if a.get('improvements'))
        
        logger.info(f"æ€§èƒ½åˆ†æå®Œæˆ:")
        logger.info(f"  æ€»æµ‹è¯•æ•°: {len(analyses)}")
        logger.info(f"  æ£€æµ‹åˆ°å›å½’: {regressions}")
        logger.info(f"  å‘ç°æ”¹è¿›: {improvements}")
        
        # å¦‚æœæ£€æµ‹åˆ°å›å½’ï¼Œè¿”å›éé›¶é€€å‡ºç 
        if regressions > 0:
            logger.warning(f"æ£€æµ‹åˆ° {regressions} ä¸ªæ€§èƒ½å›å½’ï¼")
            sys.exit(1)
        
    except Exception as e:
        logger.error(f"æ€§èƒ½åˆ†æå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()