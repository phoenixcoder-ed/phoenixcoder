{
  "issues": [
    {
      "type": "missing_script",
      "path": "security-scan.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/security-scan.py",
      "file": "ci.yml",
      "location": "jobs.security-scan.steps[2].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: security-scan.py"
    },
    {
      "type": "missing_script",
      "path": "const { data: comments } = await github.rest.issues.listComments({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  issue_number: context.issue.number,\n});\n\nconst botComment = comments.find(comment => \n  comment.user.type === 'Bot' && comment.body.includes('## ðŸ” Code Quality Report')\n);\n\nconst lintResult = '${{ needs.lint-and-format.result }}';\nconst sonarResult = '${{ needs.sonarqube-analysis.result }}';\nconst codeClimateResult = '${{ needs.codeclimate-analysis.result }}';\nconst depResult = '${{ needs.dependency-analysis.result }}';\n\nconst getStatusEmoji = (result) => {\n  switch(result) {\n    case 'success': return 'âœ…';\n    case 'failure': return 'âŒ';\n    case 'cancelled': return 'â¹ï¸';\n    case 'skipped': return 'â­ï¸';\n    default: return 'âš ï¸';\n  }\n};\n\nconst body = `## ðŸ” Code Quality Report\n\n| Check | Status | Result |\n|-------|--------|--------|\n| Lint & Format | ${getStatusEmoji(lintResult)} | ${lintResult} |\n| SonarQube | ${getStatusEmoji(sonarResult)} | ${sonarResult} |\n| CodeClimate | ${getStatusEmoji(codeClimateResult)} | ${codeClimateResult} |\n| Dependencies | ${getStatusEmoji(depResult)} | ${depResult} |\n\n**Overall Status**: ${lintResult === 'success' && depResult === 'success' ? 'âœ… Passed' : 'âŒ Failed'}\n\n---\n*Updated at: ${new Date().toISOString()}*`;\n\nif (botComment) {\n  await github.rest.issues.updateComment({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    comment_id: botComment.id,\n    body: body\n  });\n} else {\n  await github.rest.issues.createComment({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    issue_number: context.issue.number,\n    body: body\n  });\n}\n",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/const { data: comments } = await github.rest.issues.listComments({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  issue_number: context.issue.number,\n});\n\nconst botComment = comments.find(comment => \n  comment.user.type === 'Bot' && comment.body.includes('## ðŸ” Code Quality Report')\n);\n\nconst lintResult = '${{ needs.lint-and-format.result }}';\nconst sonarResult = '${{ needs.sonarqube-analysis.result }}';\nconst codeClimateResult = '${{ needs.codeclimate-analysis.result }}';\nconst depResult = '${{ needs.dependency-analysis.result }}';\n\nconst getStatusEmoji = (result) => {\n  switch(result) {\n    case 'success': return 'âœ…';\n    case 'failure': return 'âŒ';\n    case 'cancelled': return 'â¹ï¸';\n    case 'skipped': return 'â­ï¸';\n    default: return 'âš ï¸';\n  }\n};\n\nconst body = `## ðŸ” Code Quality Report\n\n| Check | Status | Result |\n|-------|--------|--------|\n| Lint & Format | ${getStatusEmoji(lintResult)} | ${lintResult} |\n| SonarQube | ${getStatusEmoji(sonarResult)} | ${sonarResult} |\n| CodeClimate | ${getStatusEmoji(codeClimateResult)} | ${codeClimateResult} |\n| Dependencies | ${getStatusEmoji(depResult)} | ${depResult} |\n\n**Overall Status**: ${lintResult === 'success' && depResult === 'success' ? 'âœ… Passed' : 'âŒ Failed'}\n\n---\n*Updated at: ${new Date().toISOString()}*`;\n\nif (botComment) {\n  await github.rest.issues.updateComment({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    comment_id: botComment.id,\n    body: body\n  });\n} else {\n  await github.rest.issues.createComment({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    issue_number: context.issue.number,\n    body: body\n  });\n}\n",
      "file": "code-quality.yml",
      "location": "jobs.quality-gate.steps[1].with.script",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: const { data: comments } = await github.rest.issues.listComments({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  issue_number: context.issue.number,\n});\n\nconst botComment = comments.find(comment => \n  comment.user.type === 'Bot' && comment.body.includes('## ðŸ” Code Quality Report')\n);\n\nconst lintResult = '${{ needs.lint-and-format.result }}';\nconst sonarResult = '${{ needs.sonarqube-analysis.result }}';\nconst codeClimateResult = '${{ needs.codeclimate-analysis.result }}';\nconst depResult = '${{ needs.dependency-analysis.result }}';\n\nconst getStatusEmoji = (result) => {\n  switch(result) {\n    case 'success': return 'âœ…';\n    case 'failure': return 'âŒ';\n    case 'cancelled': return 'â¹ï¸';\n    case 'skipped': return 'â­ï¸';\n    default: return 'âš ï¸';\n  }\n};\n\nconst body = `## ðŸ” Code Quality Report\n\n| Check | Status | Result |\n|-------|--------|--------|\n| Lint & Format | ${getStatusEmoji(lintResult)} | ${lintResult} |\n| SonarQube | ${getStatusEmoji(sonarResult)} | ${sonarResult} |\n| CodeClimate | ${getStatusEmoji(codeClimateResult)} | ${codeClimateResult} |\n| Dependencies | ${getStatusEmoji(depResult)} | ${depResult} |\n\n**Overall Status**: ${lintResult === 'success' && depResult === 'success' ? 'âœ… Passed' : 'âŒ Failed'}\n\n---\n*Updated at: ${new Date().toISOString()}*`;\n\nif (botComment) {\n  await github.rest.issues.updateComment({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    comment_id: botComment.id,\n    body: body\n  });\n} else {\n  await github.rest.issues.createComment({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    issue_number: context.issue.number,\n    body: body\n  });\n}\n"
    },
    {
      "type": "missing_script",
      "path": "tests/smoke/frontend-smoke.js",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/tests/smoke/frontend-smoke.js",
      "file": "deploy.yml",
      "location": "jobs.deploy-staging.steps[5].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: tests/smoke/frontend-smoke.js"
    },
    {
      "type": "missing_script",
      "path": "collect_data.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/collect_data.py",
      "file": "notifications.yml",
      "location": "jobs.collect-workflow-data.steps[1].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: collect_data.py"
    },
    {
      "type": "missing_script",
      "path": "send_slack.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/send_slack.py",
      "file": "notifications.yml",
      "location": "jobs.send-slack-notification.steps[0].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: send_slack.py"
    },
    {
      "type": "missing_script",
      "path": "send_email.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/send_email.py",
      "file": "notifications.yml",
      "location": "jobs.send-email-notification.steps[0].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: send_email.py"
    },
    {
      "type": "missing_script",
      "path": "update_pr.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/update_pr.py",
      "file": "notifications.yml",
      "location": "jobs.update-pr-status.steps[0].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: update_pr.py"
    },
    {
      "type": "missing_script",
      "path": "generate_badges.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/generate_badges.py",
      "file": "notifications.yml",
      "location": "jobs.generate-status-badges.steps[1].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: generate_badges.py"
    },
    {
      "type": "missing_script",
      "path": "script.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/script.py",
      "file": "performance-monitoring.yml",
      "location": "jobs.backend-performance-tests.steps[5].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: script.py"
    },
    {
      "type": "missing_script",
      "path": "performance-test-$SERVICE.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/performance-test-$SERVICE.py",
      "file": "performance-monitoring.yml",
      "location": "jobs.backend-performance-tests.steps[6].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: performance-test-$SERVICE.py"
    },
    {
      "type": "missing_script",
      "path": "extract-lighthouse-metrics.js",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/extract-lighthouse-metrics.js",
      "file": "performance-monitoring.yml",
      "location": "jobs.frontend-performance-tests.steps[5].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: extract-lighthouse-metrics.js"
    },
    {
      "type": "missing_script",
      "path": "analyze-performance.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/analyze-performance.py",
      "file": "performance-monitoring.yml",
      "location": "jobs.analyze-performance-results.steps[6].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: analyze-performance.py"
    },
    {
      "type": "missing_script",
      "path": "create-performance-report.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/create-performance-report.py",
      "file": "performance-monitoring.yml",
      "location": "jobs.analyze-performance-results.steps[7].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: create-performance-report.py"
    },
    {
      "type": "missing_script",
      "path": "const fs = require('fs');\n\ntry {\n  const reportContent = fs.readFileSync('performance-report.md', 'utf8');\n  \n  // Find existing performance comment\n  const comments = await github.rest.issues.listComments({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    issue_number: context.issue.number,\n  });\n  \n  const existingComment = comments.data.find(comment => \n    comment.body.includes('ðŸš€ Performance Monitoring Report')\n  );\n  \n  const commentBody = `${reportContent}\n  \n  ---\n  *This report was automatically generated by the Performance Monitoring workflow.*`;\n  \n  if (existingComment) {\n    await github.rest.issues.updateComment({\n      owner: context.repo.owner,\n      repo: context.repo.repo,\n      comment_id: existingComment.id,\n      body: commentBody\n    });\n  } else {\n    await github.rest.issues.createComment({\n      owner: context.repo.owner,\n      repo: context.repo.repo,\n      issue_number: context.issue.number,\n      body: commentBody\n    });\n  }\n} catch (error) {\n  console.error('Error posting performance report:', error);\n}\n",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/const fs = require('fs');\n\ntry {\n  const reportContent = fs.readFileSync('performance-report.md', 'utf8');\n  \n  / Find existing performance comment\n  const comments = await github.rest.issues.listComments({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    issue_number: context.issue.number,\n  });\n  \n  const existingComment = comments.data.find(comment => \n    comment.body.includes('ðŸš€ Performance Monitoring Report')\n  );\n  \n  const commentBody = `${reportContent}\n  \n  ---\n  *This report was automatically generated by the Performance Monitoring workflow.*`;\n  \n  if (existingComment) {\n    await github.rest.issues.updateComment({\n      owner: context.repo.owner,\n      repo: context.repo.repo,\n      comment_id: existingComment.id,\n      body: commentBody\n    });\n  } else {\n    await github.rest.issues.createComment({\n      owner: context.repo.owner,\n      repo: context.repo.repo,\n      issue_number: context.issue.number,\n      body: commentBody\n    });\n  }\n} catch (error) {\n  console.error('Error posting performance report:', error);\n}\n",
      "file": "performance-monitoring.yml",
      "location": "jobs.analyze-performance-results.steps[9].with.script",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: const fs = require('fs');\n\ntry {\n  const reportContent = fs.readFileSync('performance-report.md', 'utf8');\n  \n  // Find existing performance comment\n  const comments = await github.rest.issues.listComments({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    issue_number: context.issue.number,\n  });\n  \n  const existingComment = comments.data.find(comment => \n    comment.body.includes('ðŸš€ Performance Monitoring Report')\n  );\n  \n  const commentBody = `${reportContent}\n  \n  ---\n  *This report was automatically generated by the Performance Monitoring workflow.*`;\n  \n  if (existingComment) {\n    await github.rest.issues.updateComment({\n      owner: context.repo.owner,\n      repo: context.repo.repo,\n      comment_id: existingComment.id,\n      body: commentBody\n    });\n  } else {\n    await github.rest.issues.createComment({\n      owner: context.repo.owner,\n      repo: context.repo.repo,\n      issue_number: context.issue.number,\n      body: commentBody\n    });\n  }\n} catch (error) {\n  console.error('Error posting performance report:', error);\n}\n"
    },
    {
      "type": "missing_script",
      "path": "const artifacts = await github.rest.actions.listWorkflowRunArtifacts({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  run_id: ${{ github.event.workflow_run.id }}\n});\n\nfor (const artifact of artifacts.data.artifacts) {\n  const download = await github.rest.actions.downloadArtifact({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    artifact_id: artifact.id,\n    archive_format: 'zip'\n  });\n  \n  const fs = require('fs');\n  fs.writeFileSync(`${artifact.name}.zip`, Buffer.from(download.data));\n}\n",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/const artifacts = await github.rest.actions.listWorkflowRunArtifacts({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  run_id: ${{ github.event.workflow_run.id }}\n});\n\nfor (const artifact of artifacts.data.artifacts) {\n  const download = await github.rest.actions.downloadArtifact({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    artifact_id: artifact.id,\n    archive_format: 'zip'\n  });\n  \n  const fs = require('fs');\n  fs.writeFileSync(`${artifact.name}.zip`, Buffer.from(download.data));\n}\n",
      "file": "test-report.yml",
      "location": "jobs.generate-test-report.steps[2].with.script",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: const artifacts = await github.rest.actions.listWorkflowRunArtifacts({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  run_id: ${{ github.event.workflow_run.id }}\n});\n\nfor (const artifact of artifacts.data.artifacts) {\n  const download = await github.rest.actions.downloadArtifact({\n    owner: context.repo.owner,\n    repo: context.repo.repo,\n    artifact_id: artifact.id,\n    archive_format: 'zip'\n  });\n  \n  const fs = require('fs');\n  fs.writeFileSync(`${artifact.name}.zip`, Buffer.from(download.data));\n}\n"
    },
    {
      "type": "missing_script",
      "path": "const fs = require('fs');\n\nif (fs.existsSync('test-report.json')) {\n  const report = JSON.parse(fs.readFileSync('test-report.json', 'utf8'));\n  \n  const status = report.summary.failed > 0 ? 'âŒ å¤±è´¥' : 'âœ… é€šè¿‡';\n  const successRate = Math.round((report.summary.passed / report.summary.total) * 100);\n  \n  const message = `## ðŸ§ª æµ‹è¯•æŠ¥å‘Š ${status}\n  \n  **ðŸ“Š æµ‹è¯•ç»Ÿè®¡**\n  - æ€»æµ‹è¯•æ•°: ${report.summary.total}\n  - é€šè¿‡: ${report.summary.passed}\n  - å¤±è´¥: ${report.summary.failed}\n  - è·³è¿‡: ${report.summary.skipped}\n  - æˆåŠŸçŽ‡: ${successRate}%\n  \n  **ðŸ”§ åŽç«¯æœåŠ¡**\n  ${Object.entries(report.details.backend).map(([service, data]) => \n    `- ${service}: ${data.tests.passed}/${data.tests.total} é€šè¿‡${data.coverage ? ` (è¦†ç›–çŽ‡: ${data.coverage.lines}%)` : ''}`\n  ).join('\\n')}\n  \n  **ðŸŽ¨ å‰ç«¯åº”ç”¨**\n  ${Object.entries(report.details.frontend).map(([app, data]) => \n    `- ${app}: ${data.tests.passed}/${data.tests.total} é€šè¿‡`\n  ).join('\\n')}\n  \n  **ðŸ“… æž„å»ºä¿¡æ¯**\n  - æ—¶é—´: ${new Date(report.timestamp).toLocaleString('zh-CN')}\n  - æäº¤: ${report.commit_sha?.substring(0, 8)}\n  - åˆ†æ”¯: ${report.branch}\n  \n  ðŸ“‹ [æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})`;\n  \n  // å¦‚æžœæ˜¯PRï¼Œæ·»åŠ è¯„è®º\n  if (context.payload.workflow_run.event === 'pull_request') {\n    const pulls = await github.rest.pulls.list({\n      owner: context.repo.owner,\n      repo: context.repo.repo,\n      head: `${context.repo.owner}:${report.branch}`,\n      state: 'open'\n    });\n    \n    if (pulls.data.length > 0) {\n      await github.rest.issues.createComment({\n        issue_number: pulls.data[0].number,\n        owner: context.repo.owner,\n        repo: context.repo.repo,\n        body: message\n      });\n    }\n  }\n  \n  console.log(message);\n}\n",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/const fs = require('fs');\n\nif (fs.existsSync('test-report.json')) {\n  const report = JSON.parse(fs.readFileSync('test-report.json', 'utf8'));\n  \n  const status = report.summary.failed > 0 ? 'âŒ å¤±è´¥' : 'âœ… é€šè¿‡';\n  const successRate = Math.round((report.summary.passed / report.summary.total) * 100);\n  \n  const message = `## ðŸ§ª æµ‹è¯•æŠ¥å‘Š ${status}\n  \n  **ðŸ“Š æµ‹è¯•ç»Ÿè®¡**\n  - æ€»æµ‹è¯•æ•°: ${report.summary.total}\n  - é€šè¿‡: ${report.summary.passed}\n  - å¤±è´¥: ${report.summary.failed}\n  - è·³è¿‡: ${report.summary.skipped}\n  - æˆåŠŸçŽ‡: ${successRate}%\n  \n  **ðŸ”§ åŽç«¯æœåŠ¡**\n  ${Object.entries(report.details.backend).map(([service, data]) => \n    `- ${service}: ${data.tests.passed}/${data.tests.total} é€šè¿‡${data.coverage ? ` (è¦†ç›–çŽ‡: ${data.coverage.lines}%)` : ''}`\n  ).join('\\n')}\n  \n  **ðŸŽ¨ å‰ç«¯åº”ç”¨**\n  ${Object.entries(report.details.frontend).map(([app, data]) => \n    `- ${app}: ${data.tests.passed}/${data.tests.total} é€šè¿‡`\n  ).join('\\n')}\n  \n  **ðŸ“… æž„å»ºä¿¡æ¯**\n  - æ—¶é—´: ${new Date(report.timestamp).toLocaleString('zh-CN')}\n  - æäº¤: ${report.commit_sha?.substring(0, 8)}\n  - åˆ†æ”¯: ${report.branch}\n  \n  ðŸ“‹ [æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})`;\n  \n  / å¦‚æžœæ˜¯PRï¼Œæ·»åŠ è¯„è®º\n  if (context.payload.workflow_run.event === 'pull_request') {\n    const pulls = await github.rest.pulls.list({\n      owner: context.repo.owner,\n      repo: context.repo.repo,\n      head: `${context.repo.owner}:${report.branch}`,\n      state: 'open'\n    });\n    \n    if (pulls.data.length > 0) {\n      await github.rest.issues.createComment({\n        issue_number: pulls.data[0].number,\n        owner: context.repo.owner,\n        repo: context.repo.repo,\n        body: message\n      });\n    }\n  }\n  \n  console.log(message);\n}\n",
      "file": "test-report.yml",
      "location": "jobs.generate-test-report.steps[7].with.script",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: const fs = require('fs');\n\nif (fs.existsSync('test-report.json')) {\n  const report = JSON.parse(fs.readFileSync('test-report.json', 'utf8'));\n  \n  const status = report.summary.failed > 0 ? 'âŒ å¤±è´¥' : 'âœ… é€šè¿‡';\n  const successRate = Math.round((report.summary.passed / report.summary.total) * 100);\n  \n  const message = `## ðŸ§ª æµ‹è¯•æŠ¥å‘Š ${status}\n  \n  **ðŸ“Š æµ‹è¯•ç»Ÿè®¡**\n  - æ€»æµ‹è¯•æ•°: ${report.summary.total}\n  - é€šè¿‡: ${report.summary.passed}\n  - å¤±è´¥: ${report.summary.failed}\n  - è·³è¿‡: ${report.summary.skipped}\n  - æˆåŠŸçŽ‡: ${successRate}%\n  \n  **ðŸ”§ åŽç«¯æœåŠ¡**\n  ${Object.entries(report.details.backend).map(([service, data]) => \n    `- ${service}: ${data.tests.passed}/${data.tests.total} é€šè¿‡${data.coverage ? ` (è¦†ç›–çŽ‡: ${data.coverage.lines}%)` : ''}`\n  ).join('\\n')}\n  \n  **ðŸŽ¨ å‰ç«¯åº”ç”¨**\n  ${Object.entries(report.details.frontend).map(([app, data]) => \n    `- ${app}: ${data.tests.passed}/${data.tests.total} é€šè¿‡`\n  ).join('\\n')}\n  \n  **ðŸ“… æž„å»ºä¿¡æ¯**\n  - æ—¶é—´: ${new Date(report.timestamp).toLocaleString('zh-CN')}\n  - æäº¤: ${report.commit_sha?.substring(0, 8)}\n  - åˆ†æ”¯: ${report.branch}\n  \n  ðŸ“‹ [æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})`;\n  \n  // å¦‚æžœæ˜¯PRï¼Œæ·»åŠ è¯„è®º\n  if (context.payload.workflow_run.event === 'pull_request') {\n    const pulls = await github.rest.pulls.list({\n      owner: context.repo.owner,\n      repo: context.repo.repo,\n      head: `${context.repo.owner}:${report.branch}`,\n      state: 'open'\n    });\n    \n    if (pulls.data.length > 0) {\n      await github.rest.issues.createComment({\n        issue_number: pulls.data[0].number,\n        owner: context.repo.owner,\n        repo: context.repo.repo,\n        body: message\n      });\n    }\n  }\n  \n  console.log(message);\n}\n"
    },
    {
      "type": "missing_script",
      "path": "manage.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/manage.py",
      "file": "test.yml",
      "location": "jobs.backend-unit-tests.steps[3].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: manage.py"
    },
    {
      "type": "missing_script",
      "path": "scripts/setup-test-db.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/scripts/setup-test-db.py",
      "file": "test.yml",
      "location": "jobs.integration-tests.steps[5].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: scripts/setup-test-db.py"
    },
    {
      "type": "missing_script",
      "path": "scripts/setup-test-db.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/scripts/setup-test-db.py",
      "file": "test.yml",
      "location": "jobs.e2e-tests.steps[6].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: scripts/setup-test-db.py"
    },
    {
      "type": "missing_script",
      "path": "scripts/setup-test-db.py",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/scripts/setup-test-db.py",
      "file": "test.yml",
      "location": "jobs.performance-tests.steps[6].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: scripts/setup-test-db.py"
    },
    {
      "type": "missing_script",
      "path": "./run-performance-tests.sh",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/run-performance-tests.sh",
      "file": "test.yml",
      "location": "jobs.performance-tests.steps[8].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: ./run-performance-tests.sh"
    },
    {
      "type": "missing_script",
      "path": "run-performance-tests.sh",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/run-performance-tests.sh",
      "file": "test.yml",
      "location": "jobs.performance-tests.steps[8].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: run-performance-tests.sh"
    },
    {
      "type": "missing_script",
      "path": "scripts/generate-report.js",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/scripts/generate-report.js",
      "file": "test.yml",
      "location": "jobs.performance-tests.steps[9].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: scripts/generate-report.js"
    },
    {
      "type": "missing_script",
      "path": "scripts/update-benchmarks.js",
      "fullPath": "/Users/zhuwencan/work/phoenixcoder/scripts/update-benchmarks.js",
      "file": "test.yml",
      "location": "jobs.performance-tests.steps[10].run",
      "severity": "error",
      "message": "è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: scripts/update-benchmarks.js"
    }
  ],
  "scriptReferences": [
    {
      "file": "ci.yml",
      "path": "jobs.security-scan.steps[2].run",
      "scriptPath": "security-scan.py",
      "command": "cat > security-scan.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport sys\nimport json\nimport subprocess\n\ndef scan_docker_images():\n    \"\"\"æ‰«æDockeré•œåƒä¸­çš„æ¼æ´ž\"\"\"\n    print(\"ðŸ” æ‰«æDockeré•œåƒæ¼æ´ž...\")\n\n    vulnerabilities = []\n\n    # èŽ·å–æ‰€æœ‰Dockerfile\n    dockerfiles = []\n    for root, dirs, files in os.walk('.'):\n        for file in files:\n            if file == 'Dockerfile':\n                dockerfiles.append(os.path.join(root, file))\n\n    if not dockerfiles:\n        print(\"âš ï¸ æœªæ‰¾åˆ°Dockerfile\")\n        return vulnerabilities\n\n    print(f\"ðŸ“‹ æ‰¾åˆ° {len(dockerfiles)} ä¸ªDockerfile\")\n\n    # æ¨¡æ‹Ÿæ‰«æç»“æžœ\n    # åœ¨å®žé™…çŽ¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨Dockerå®‰å…¨æ‰«æå·¥å…·ï¼Œå¦‚Trivy\n    # ä¾‹å¦‚ï¼šsubprocess.run(['trivy', 'image', image_name])\n\n    # ç¤ºä¾‹æ¼æ´žæ•°æ®\n    sample_vulnerabilities = [\n        {\n            \"VulnerabilityID\": \"CVE-2023-1234\",\n            \"PkgName\": \"openssl\",\n            \"InstalledVersion\": \"1.1.1k\",\n            \"FixedVersion\": \"1.1.1l\",\n            \"Severity\": \"HIGH\",\n            \"Description\": \"OpenSSL å®‰å…¨æ¼æ´ž\"\n        },\n        {\n            \"VulnerabilityID\": \"CVE-2023-5678\",\n            \"PkgName\": \"nodejs\",\n            \"InstalledVersion\": \"14.17.0\",\n            \"FixedVersion\": \"14.17.5\",\n            \"Severity\": \"MEDIUM\",\n            \"Description\": \"Node.js å®‰å…¨æ¼æ´ž\"\n        }\n    ]\n\n    # ä¸ºæ¯ä¸ªDockerfileæ·»åŠ ä¸€äº›æ¨¡æ‹Ÿæ¼æ´ž\n    for dockerfile in dockerfiles:\n        for vuln in sample_vulnerabilities:\n            vuln_copy = vuln.copy()\n            vuln_copy[\"Dockerfile\"] = dockerfile\n            vulnerabilities.append(vuln_copy)\n\n    print(f\"ðŸ” å‘çŽ° {len(vulnerabilities)} ä¸ªDockeré•œåƒæ¼æ´ž\")\n    return vulnerabilities\n\ndef scan_dependencies():\n    \"\"\"æ‰«æä¾èµ–åŒ…ä¸­çš„æ¼æ´ž\"\"\"\n    print(\"ðŸ” æ‰«æä¾èµ–åŒ…æ¼æ´ž...\")\n\n    vulnerabilities = []\n\n    # æ£€æŸ¥package.json\n    if os.path.exists('package.json'):\n        print(\"ðŸ“¦ æ‰«æNode.jsä¾èµ–...\")\n        try:\n            # åœ¨å®žé™…çŽ¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨npm audit\n            # ä¾‹å¦‚ï¼šresult = subprocess.run(['npm', 'audit', '--json'], capture_output=True, text=True)\n            # ç„¶åŽè§£æžJSONè¾“å‡º\n\n            # æ¨¡æ‹Ÿä¸€äº›Node.jsä¾èµ–æ¼æ´ž\n            node_vulnerabilities = [\n                {\n                    \"id\": 1234,\n                    \"name\": \"lodash\",\n                    \"version\": \"4.17.15\",\n                    \"severity\": \"high\",\n                    \"recommendation\": \"Upgrade to version 4.17.21 or later\"\n                },\n                {\n                    \"id\": 5678,\n                    \"name\": \"minimist\",\n                    \"version\": \"1.2.5\",\n                    \"severity\": \"medium\",\n                    \"recommendation\": \"Upgrade to version 1.2.6 or later\"\n                }\n            ]\n\n            vulnerabilities.extend(node_vulnerabilities)\n            print(f\"    ðŸ“‹ å‘çŽ° {len(node_vulnerabilities)} ä¸ªNode.jsä¾èµ–æ¼æ´ž\")\n        except Exception as e:\n            print(f\"    âš ï¸ Node.jsä¾èµ–æ‰«æå¤±è´¥: {e}\")\n\n    # æ£€æŸ¥requirements.txt\n    if os.path.exists('requirements.txt'):\n        print(\"ðŸ“¦ æ‰«æPythonä¾èµ–...\")\n        try:\n            # åœ¨å®žé™…çŽ¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨pip-auditæˆ–safety\n            # ä¾‹å¦‚ï¼šresult = subprocess.run(['pip-audit', '-r', 'requirements.txt', '--format', 'json'], capture_output=True, text=True)\n\n            # æ¨¡æ‹Ÿä¸€äº›Pythonä¾èµ–æ¼æ´ž\n            python_vulnerabilities = [\n                {\n                    \"name\": \"django\",\n                    \"installed_version\": \"3.2.0\",\n                    \"vulnerable_version\": \"<3.2.4\",\n                    \"severity\": \"high\",\n                    \"advisory\": \"CVE-2021-33203\"\n                },\n                {\n                    \"name\": \"pillow\",\n                    \"installed_version\": \"8.2.0\",\n                    \"vulnerable_version\": \"<8.3.0\",\n                    \"severity\": \"medium\",\n                    \"advisory\": \"CVE-2021-34552\"\n                }\n            ]\n\n            vulnerabilities.extend(python_vulnerabilities)\n            print(f\"    ðŸ“‹ å‘çŽ° {len(python_vulnerabilities)} ä¸ªPythonä¾èµ–æ¼æ´ž\")\n        except Exception as e:\n            print(f\"    âš ï¸ Pythonä¾èµ–æ‰«æå¤±è´¥: {e}\")\n\n    # æ£€æŸ¥å…¶ä»–æœåŠ¡çš„ä¾èµ–\n    services_dir = ['services', 'apps']\n    for service_dir in services_dir:\n        if os.path.exists(service_dir):\n            for service in os.listdir(service_dir):\n                service_path = os.path.join(service_dir, service)\n                if os.path.isdir(service_path):\n                    # æ£€æŸ¥æœåŠ¡çš„package.json\n                    if os.path.exists(os.path.join(service_path, 'package.json')):\n                        print(f\"ðŸ“¦ æ‰«æ {service} æœåŠ¡çš„Node.jsä¾èµ–...\")\n                        try:\n                            # æ¨¡æ‹Ÿä¸€äº›æœåŠ¡ç‰¹å®šçš„ä¾èµ–æ¼æ´ž\n                            service_vulnerabilities = [\n                                {\n                                    \"id\": 9012,\n                                    \"name\": \"express\",\n                                    \"version\": \"4.17.1\",\n                                    \"severity\": \"low\",\n                                    \"service\": service,\n                                    \"recommendation\": \"Upgrade to version 4.17.3 or later\"\n                                }\n                            ]\n\n                            vulnerabilities.extend(service_vulnerabilities)\n                            print(f\"    ðŸ“‹ å‘çŽ° {len(service_vulnerabilities)} ä¸ª {service} æœåŠ¡Node.jsä¾èµ–æ¼æ´ž\")\n                        except Exception as e:\n                            print(f\"    âš ï¸ {service}æœåŠ¡Node.jsä¾èµ–æ‰«æå¤±è´¥: {e}\")\n\n                    # æ£€æŸ¥æœåŠ¡çš„requirements.txt\n                    if os.path.exists(os.path.join(service_path, 'requirements.txt')):\n                        print(f\"ðŸ“¦ æ‰«æ {service} æœåŠ¡çš„Pythonä¾èµ–...\")\n                        try:\n                            # æ¨¡æ‹Ÿä¸€äº›æœåŠ¡ç‰¹å®šçš„Pythonä¾èµ–æ¼æ´ž\n                            service_vulnerabilities = [\n                                {\n                                    \"name\": \"flask\",\n                                    \"installed_version\": \"2.0.0\",\n                                    \"vulnerable_version\": \"<2.0.1\",\n                                    \"severity\": \"medium\",\n                                    \"service\": service,\n                                    \"advisory\": \"CVE-2021-12345\"\n                                }\n                            ]\n\n                            vulnerabilities.extend(service_vulnerabilities)\n                            print(f\"    ðŸ“‹ å‘çŽ° {len(service_vulnerabilities)} ä¸ª {service} æœåŠ¡Pythonä¾èµ–æ¼æ´ž\")\n                        except Exception as e:\n                            print(f\"    âš ï¸ {service}æœåŠ¡Pythonä¾èµ–æ‰«æå¤±è´¥: {e}\")\n\n    return vulnerabilities\n\ndef scan_secrets():\n    \"\"\"æ‰«æä»£ç ä¸­çš„æ•æ„Ÿä¿¡æ¯\"\"\"\n    print(\"ðŸ” æ‰«æä»£ç ä¸­çš„æ•æ„Ÿä¿¡æ¯...\")\n\n    secrets = []\n\n    # ä½¿ç”¨ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼æ‰«æ\n    import re\n\n    secret_patterns = [\n          (r'password\\s*=\\s*[\"\\'][^\"\\'\\r\\n]{8,}[\"\\']', 'hardcoded_password'),\n          (r'api[_-]?key\\s*=\\s*[\"\\'][^\"\\'\\r\\n]{20,}[\"\\']', 'api_key'),\n          (r'secret[_-]?key\\s*=\\s*[\"\\'][^\"\\'\\r\\n]{20,}[\"\\']', 'secret_key'),\n          (r'token\\s*=\\s*[\"\\'][^\"\\'\\r\\n]{20,}[\"\\']', 'token'),\n          (r'-----BEGIN [A-Z ]+-----', 'private_key')\n      ]\n\n    for root, dirs, files in os.walk('.'):\n        # è·³è¿‡ä¸éœ€è¦æ‰«æçš„ç›®å½•\n        dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['node_modules', '__pycache__', 'dist', 'build']]\n\n        for file in files:\n            if file.endswith(('.py', '.js', '.ts', '.jsx', '.tsx', '.env', '.yaml', '.yml')):\n                file_path = os.path.join(root, file)\n                try:\n                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                        content = f.read()\n\n                        for pattern, secret_type in secret_patterns:\n                            matches = re.finditer(pattern, content, re.IGNORECASE)\n                            for match in matches:\n                                secrets.append({\n                                    'file': file_path,\n                                    'type': secret_type,\n                                    'line': content[:match.start()].count('\\n') + 1,\n                                    'match': match.group()[:50] + '...' if len(match.group()) > 50 else match.group()\n                                })\n                except Exception as e:\n                    continue\n\n    return secrets\n\ndef main():\n    environment = os.environ.get('ENVIRONMENT', 'staging')\n\n    print(f\"ðŸ”’ å¼€å§‹ {environment} çŽ¯å¢ƒå®‰å…¨æ‰«æ...\")\n\n    # æ‰«æç»“æžœ\n    scan_results = {\n        'environment': environment,\n        'docker_vulnerabilities': [],\n        'dependency_vulnerabilities': [],\n        'secrets': []\n    }\n\n    # æ‰«æDockeré•œåƒ\n    try:\n        scan_results['docker_vulnerabilities'] = scan_docker_images()\n    except Exception as e:\n        print(f\"âŒ Dockeré•œåƒæ‰«æå¤±è´¥: {e}\")\n\n    # æ‰«æä¾èµ–åŒ…\n    try:\n        scan_results['dependency_vulnerabilities'] = scan_dependencies()\n    except Exception as e:\n        print(f\"âŒ ä¾èµ–åŒ…æ‰«æå¤±è´¥: {e}\")\n\n    # æ‰«ææ•æ„Ÿä¿¡æ¯\n    try:\n        scan_results['secrets'] = scan_secrets()\n    except Exception as e:\n        print(f\"âŒ æ•æ„Ÿä¿¡æ¯æ‰«æå¤±è´¥: {e}\")\n\n    # ä¿å­˜æ‰«æç»“æžœ\n    os.makedirs('security-reports', exist_ok=True)\n    report_file = f'security-reports/{environment}-security-scan.json'\n\n    with open(report_file, 'w') as f:\n        json.dump(scan_results, f, indent=2)\n\n    print(f\"ðŸ“ å®‰å…¨æ‰«ææŠ¥å‘Šå·²ä¿å­˜åˆ° {report_file}\")\n\n    # è¾“å‡ºæ‘˜è¦\n    docker_high_critical = len([v for v in scan_results['docker_vulnerabilities'] if v.get('Severity') in ['HIGH', 'CRITICAL']])\n    dep_high_critical = len([v for v in scan_results['dependency_vulnerabilities'] if v.get('severity') in ['high', 'critical']])\n    secrets_count = len(scan_results['secrets'])\n\n    print(f\"\\nðŸ“Š å®‰å…¨æ‰«ææ‘˜è¦:\")\n    print(f\"  Dockeré«˜å±æ¼æ´ž: {docker_high_critical}\")\n    print(f\"  ä¾èµ–åŒ…é«˜å±æ¼æ´ž: {dep_high_critical}\")\n    print(f\"  æ•æ„Ÿä¿¡æ¯æ³„éœ²: {secrets_count}\")\n\n    # æ£€æŸ¥æ˜¯å¦æœ‰é˜»å¡žæ€§å®‰å…¨é—®é¢˜\n    if docker_high_critical > 10 or dep_high_critical > 5 or secrets_count > 0:\n        print(f\"\\nâŒ å‘çŽ°ä¸¥é‡å®‰å…¨é—®é¢˜ï¼Œå»ºè®®ä¿®å¤åŽå†éƒ¨ç½²\")\n        if environment == 'production':\n            print(f\"ðŸš« ç”Ÿäº§çŽ¯å¢ƒéƒ¨ç½²è¢«é˜»æ­¢\")\n            sys.exit(1)\n        else:\n            print(f\"âš ï¸ æµ‹è¯•çŽ¯å¢ƒå…è®¸éƒ¨ç½²ï¼Œä½†è¯·å°½å¿«ä¿®å¤\")\n    else:\n        print(f\"\\nâœ… å®‰å…¨æ‰«æé€šè¿‡\")\n\nif __name__ == '__main__':\n    main()\nEOF\n\n# å®‰è£…å®‰å…¨æ‰«æå·¥å…·\npip install pip-audit\n\n# è¿è¡Œå®‰å…¨æ‰«æ\npython security-scan.py"
    },
    {
      "file": "deploy.yml",
      "path": "jobs.deploy-staging.steps[5].run",
      "scriptPath": "tests/smoke/frontend-smoke.js",
      "command": "echo \"Running smoke tests against staging environment\"\n\n# Install test dependencies\nnpm install -g newman\n\n# Run API smoke tests if Postman collection exists\nif [[ -f \"tests/smoke/api-smoke-tests.postman_collection.json\" ]]; then\n  newman run tests/smoke/api-smoke-tests.postman_collection.json \\\n    --env-var \"base_url=https://staging-api-community.phoenixcoder.dev\" \\\n    --reporters cli,json \\\n    --reporter-json-export smoke-test-results.json\nfi\n\n# Run frontend smoke tests\nif [[ -f \"tests/smoke/frontend-smoke.js\" ]]; then\n  node tests/smoke/frontend-smoke.js\nfi"
    },
    {
      "file": "notifications.yml",
      "path": "jobs.collect-workflow-data.steps[1].run",
      "scriptPath": "collect_data.py",
      "command": "# Create a script to collect workflow data\ncat > collect_data.py << 'EOF'\nimport json\nimport requests\nimport os\nfrom datetime import datetime, timedelta\n\ndef get_workflow_runs(repo, token, days=1):\n    headers = {\n        'Authorization': f'token {token}',\n        'Accept': 'application/vnd.github.v3+json'\n    }\n    \n    since = (datetime.now() - timedelta(days=days)).isoformat()\n    url = f'https://api.github.com/repos/{repo}/actions/runs'\n    params = {'created': f'>{since}', 'per_page': 100}\n    \n    response = requests.get(url, headers=headers, params=params)\n    return response.json().get('workflow_runs', [])\n\ndef analyze_workflows(runs):\n    stats = {\n        'total_runs': len(runs),\n        'successful_runs': 0,\n        'failed_runs': 0,\n        'cancelled_runs': 0,\n        'workflows': {},\n        'branches': {},\n        'authors': {}\n    }\n    \n    for run in runs:\n        # Count by status\n        if run['conclusion'] == 'success':\n            stats['successful_runs'] += 1\n        elif run['conclusion'] == 'failure':\n            stats['failed_runs'] += 1\n        elif run['conclusion'] == 'cancelled':\n            stats['cancelled_runs'] += 1\n        \n        # Count by workflow\n        workflow_name = run['name']\n        if workflow_name not in stats['workflows']:\n            stats['workflows'][workflow_name] = {'success': 0, 'failure': 0, 'cancelled': 0}\n        stats['workflows'][workflow_name][run['conclusion']] += 1\n        \n        # Count by branch\n        branch = run['head_branch']\n        if branch not in stats['branches']:\n            stats['branches'][branch] = {'success': 0, 'failure': 0, 'cancelled': 0}\n        stats['branches'][branch][run['conclusion']] += 1\n        \n        # Count by author\n        author = run['head_commit']['author']['name'] if run['head_commit'] else 'Unknown'\n        if author not in stats['authors']:\n            stats['authors'][author] = {'success': 0, 'failure': 0, 'cancelled': 0}\n        stats['authors'][author][run['conclusion']] += 1\n    \n    return stats\n\n# Main execution\nrepo = os.environ['GITHUB_REPOSITORY']\ntoken = os.environ['GITHUB_TOKEN']\n\nruns = get_workflow_runs(repo, token)\nstats = analyze_workflows(runs)\n\nprint(json.dumps(stats, indent=2))\n\n# Save to output\nwith open('workflow_data.json', 'w') as f:\n    json.dump(stats, f)\nEOF\n\npython collect_data.py\n\n# Set output\necho \"workflow_data=$(cat workflow_data.json | jq -c .)\" >> $GITHUB_OUTPUT"
    },
    {
      "file": "notifications.yml",
      "path": "jobs.send-slack-notification.steps[0].run",
      "scriptPath": "send_slack.py",
      "command": "if [[ -z \"$SLACK_WEBHOOK_URL\" ]]; then\n  echo \"Slack webhook URL not configured, skipping Slack notification\"\n  exit 0\nfi\n\n# Create Slack notification script\ncat > send_slack.py << 'EOF'\nimport json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef get_status_emoji(status):\n    if status == 'success':\n        return ':white_check_mark:'\n    elif status == 'failure':\n        return ':x:'\n    elif status == 'cancelled':\n        return ':warning:'\n    else:\n        return ':question:'\n\ndef get_environment_emoji(env):\n    if env == 'production':\n        return ':rocket:'\n    elif env == 'staging':\n        return ':construction:'\n    else:\n        return ':hammer_and_wrench:'\n\ndef create_workflow_completion_message():\n    status = os.environ.get('WORKFLOW_STATUS', 'unknown')\n    workflow_name = os.environ.get('WORKFLOW_NAME', 'Unknown Workflow')\n    branch = os.environ.get('BRANCH', 'unknown')\n    author = os.environ.get('AUTHOR', 'Unknown')\n    commit_sha = os.environ.get('COMMIT_SHA', '')[:8]\n    environment = os.environ.get('ENVIRONMENT', 'unknown')\n    \n    status_emoji = get_status_emoji(status)\n    env_emoji = get_environment_emoji(environment)\n    \n    color = 'good' if status == 'success' else 'danger' if status == 'failure' else 'warning'\n    \n    return {\n        'text': f'{status_emoji} Workflow {status}: {workflow_name}',\n        'attachments': [{\n            'color': color,\n            'fields': [\n                {'title': 'Workflow', 'value': workflow_name, 'short': True},\n                {'title': 'Status', 'value': f'{status_emoji} {status.title()}', 'short': True},\n                {'title': 'Branch', 'value': branch, 'short': True},\n                {'title': 'Environment', 'value': f'{env_emoji} {environment}', 'short': True},\n                {'title': 'Author', 'value': author, 'short': True},\n                {'title': 'Commit', 'value': commit_sha, 'short': True}\n            ],\n            'footer': 'GitHub Actions',\n            'ts': int(datetime.now().timestamp())\n        }]\n    }\n\ndef create_daily_summary_message():\n    workflow_data = json.loads(os.environ.get('WORKFLOW_DATA', '{}'))\n    \n    total_runs = workflow_data.get('total_runs', 0)\n    successful_runs = workflow_data.get('successful_runs', 0)\n    failed_runs = workflow_data.get('failed_runs', 0)\n    \n    success_rate = (successful_runs / total_runs * 100) if total_runs > 0 else 0\n    \n    color = 'good' if success_rate >= 90 else 'warning' if success_rate >= 70 else 'danger'\n    \n    # Top workflows\n    workflows = workflow_data.get('workflows', {})\n    top_workflows = sorted(workflows.items(), key=lambda x: sum(x[1].values()), reverse=True)[:5]\n    \n    workflow_summary = '\\n'.join([\n        f'â€¢ {name}: {data[\"success\"]}âœ… {data[\"failure\"]}âŒ {data[\"cancelled\"]}âš ï¸'\n        for name, data in top_workflows\n    ])\n    \n    return {\n        'text': f':chart_with_upwards_trend: Daily CI/CD Summary - {success_rate:.1f}% Success Rate',\n        'attachments': [{\n            'color': color,\n            'fields': [\n                {'title': 'Total Runs', 'value': str(total_runs), 'short': True},\n                {'title': 'Success Rate', 'value': f'{success_rate:.1f}%', 'short': True},\n                {'title': 'Successful', 'value': f':white_check_mark: {successful_runs}', 'short': True},\n                {'title': 'Failed', 'value': f':x: {failed_runs}', 'short': True},\n                {'title': 'Top Workflows', 'value': workflow_summary or 'No workflows found', 'short': False}\n            ],\n            'footer': 'GitHub Actions Daily Summary',\n            'ts': int(datetime.now().timestamp())\n        }]\n    }\n\ndef create_pull_request_message():\n    pr_number = os.environ.get('PR_NUMBER', '')\n    branch = os.environ.get('BRANCH', 'unknown')\n    author = os.environ.get('AUTHOR', 'Unknown')\n    \n    return {\n        'text': f':git-pull-request: Pull Request #{pr_number} opened',\n        'attachments': [{\n            'color': 'good',\n            'fields': [\n                {'title': 'PR Number', 'value': f'#{pr_number}', 'short': True},\n                {'title': 'Branch', 'value': branch, 'short': True},\n                {'title': 'Author', 'value': author, 'short': True}\n            ],\n            'footer': 'GitHub Pull Request',\n            'ts': int(datetime.now().timestamp())\n        }]\n    }\n\ndef create_release_message():\n    author = os.environ.get('AUTHOR', 'Unknown')\n    branch = os.environ.get('BRANCH', 'main')\n    \n    return {\n        'text': f':rocket: New release published!',\n        'attachments': [{\n            'color': 'good',\n            'fields': [\n                {'title': 'Released by', 'value': author, 'short': True},\n                {'title': 'Branch', 'value': branch, 'short': True}\n            ],\n            'footer': 'GitHub Release',\n            'ts': int(datetime.now().timestamp())\n        }]\n    }\n\n# Main execution\nnotification_type = os.environ.get('NOTIFICATION_TYPE', 'unknown')\nwebhook_url = os.environ.get('SLACK_WEBHOOK_URL')\n\nif notification_type == 'workflow_completion':\n    message = create_workflow_completion_message()\nelif notification_type == 'daily_summary':\n    message = create_daily_summary_message()\nelif notification_type == 'pull_request':\n    message = create_pull_request_message()\nelif notification_type == 'release':\n    message = create_release_message()\nelse:\n    message = {'text': f'Unknown notification type: {notification_type}'}\n\n# Send to Slack\nresponse = requests.post(webhook_url, json=message)\n\nif response.status_code == 200:\n    print('Slack notification sent successfully')\nelse:\n    print(f'Failed to send Slack notification: {response.status_code} - {response.text}')\n    exit(1)\nEOF\n\npython send_slack.py"
    },
    {
      "file": "notifications.yml",
      "path": "jobs.send-email-notification.steps[0].run",
      "scriptPath": "send_email.py",
      "command": "if [[ -z \"$EMAIL_SMTP_HOST\" || -z \"$EMAIL_USERNAME\" || -z \"$EMAIL_PASSWORD\" ]]; then\n  echo \"Email configuration not complete, skipping email notification\"\n  exit 0\nfi\n\n# Install required packages\npip install jinja2\n\n# Create email notification script\ncat > send_email.py << 'EOF'\nimport smtplib\nimport json\nimport os\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom datetime import datetime\nfrom jinja2 import Template\n\ndef create_html_template():\n    return Template('''\n<!DOCTYPE html>\n<html>\n<head>\n    <meta charset=\"utf-8\">\n    <style>\n        body { font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }\n        .container { max-width: 600px; margin: 0 auto; background-color: white; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }\n        .header { background-color: {% if status == 'success' %}#28a745{% elif status == 'failure' %}#dc3545{% else %}#ffc107{% endif %}; color: white; padding: 20px; text-align: center; }\n        .content { padding: 20px; }\n        .status-badge { display: inline-block; padding: 4px 8px; border-radius: 4px; font-size: 12px; font-weight: bold; }\n        .success { background-color: #d4edda; color: #155724; }\n        .failure { background-color: #f8d7da; color: #721c24; }\n        .warning { background-color: #fff3cd; color: #856404; }\n        .info-table { width: 100%; border-collapse: collapse; margin: 20px 0; }\n        .info-table th, .info-table td { padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }\n        .info-table th { background-color: #f8f9fa; font-weight: bold; }\n        .footer { background-color: #f8f9fa; padding: 15px; text-align: center; font-size: 12px; color: #6c757d; }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"header\">\n            <h1>{{ title }}</h1>\n            <p>{{ subtitle }}</p>\n        </div>\n        <div class=\"content\">\n            {% if notification_type == 'workflow_completion' %}\n                <h2>Workflow Details</h2>\n                <table class=\"info-table\">\n                    <tr><th>Workflow</th><td>{{ workflow_name }}</td></tr>\n                    <tr><th>Status</th><td><span class=\"status-badge {{ status }}\">{{ status.title() }}</span></td></tr>\n                    <tr><th>Branch</th><td>{{ branch }}</td></tr>\n                    <tr><th>Environment</th><td>{{ environment }}</td></tr>\n                    <tr><th>Author</th><td>{{ author }}</td></tr>\n                    <tr><th>Commit</th><td>{{ commit_sha }}</td></tr>\n                    <tr><th>Timestamp</th><td>{{ timestamp }}</td></tr>\n                </table>\n            {% elif notification_type == 'daily_summary' %}\n                <h2>CI/CD Summary</h2>\n                <table class=\"info-table\">\n                    <tr><th>Total Runs</th><td>{{ workflow_data.total_runs }}</td></tr>\n                    <tr><th>Successful</th><td><span class=\"status-badge success\">{{ workflow_data.successful_runs }}</span></td></tr>\n                    <tr><th>Failed</th><td><span class=\"status-badge failure\">{{ workflow_data.failed_runs }}</span></td></tr>\n                    <tr><th>Cancelled</th><td><span class=\"status-badge warning\">{{ workflow_data.cancelled_runs }}</span></td></tr>\n                    <tr><th>Success Rate</th><td>{{ \"%.1f\" | format((workflow_data.successful_runs / workflow_data.total_runs * 100) if workflow_data.total_runs > 0 else 0) }}%</td></tr>\n                </table>\n                \n                <h3>Top Workflows</h3>\n                <table class=\"info-table\">\n                    <tr><th>Workflow</th><th>Success</th><th>Failed</th><th>Cancelled</th></tr>\n                    {% for name, data in top_workflows %}\n                    <tr>\n                        <td>{{ name }}</td>\n                        <td><span class=\"status-badge success\">{{ data.success }}</span></td>\n                        <td><span class=\"status-badge failure\">{{ data.failure }}</span></td>\n                        <td><span class=\"status-badge warning\">{{ data.cancelled }}</span></td>\n                    </tr>\n                    {% endfor %}\n                </table>\n            {% endif %}\n        </div>\n        <div class=\"footer\">\n            <p>This is an automated notification from GitHub Actions</p>\n            <p>Generated at {{ timestamp }}</p>\n        </div>\n    </div>\n</body>\n</html>\n    ''')\n\ndef send_email(subject, html_content, text_content):\n    smtp_host = os.environ.get('EMAIL_SMTP_HOST')\n    smtp_port = int(os.environ.get('EMAIL_SMTP_PORT', '587'))\n    username = os.environ.get('EMAIL_USERNAME')\n    password = os.environ.get('EMAIL_PASSWORD')\n    from_email = os.environ.get('EMAIL_FROM', username)\n    to_email = os.environ.get('EMAIL_TO')\n    \n    msg = MIMEMultipart('alternative')\n    msg['Subject'] = subject\n    msg['From'] = from_email\n    msg['To'] = to_email\n    \n    # Add text and HTML parts\n    text_part = MIMEText(text_content, 'plain')\n    html_part = MIMEText(html_content, 'html')\n    \n    msg.attach(text_part)\n    msg.attach(html_part)\n    \n    # Send email\n    with smtplib.SMTP(smtp_host, smtp_port) as server:\n        server.starttls()\n        server.login(username, password)\n        server.send_message(msg)\n\n# Main execution\nnotification_type = os.environ.get('NOTIFICATION_TYPE', 'unknown')\nworkflow_name = os.environ.get('WORKFLOW_NAME', 'Unknown Workflow')\nstatus = os.environ.get('WORKFLOW_STATUS', 'unknown')\nbranch = os.environ.get('BRANCH', 'unknown')\nauthor = os.environ.get('AUTHOR', 'Unknown')\ncommit_sha = os.environ.get('COMMIT_SHA', '')[:8]\nenvironment = os.environ.get('ENVIRONMENT', 'unknown')\ntimestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')\n\ntemplate = create_html_template()\n\nif notification_type == 'workflow_completion':\n    title = f'Workflow {status.title()}: {workflow_name}'\n    subtitle = f'Branch: {branch} | Environment: {environment}'\n    \n    html_content = template.render(\n        title=title,\n        subtitle=subtitle,\n        notification_type=notification_type,\n        workflow_name=workflow_name,\n        status=status,\n        branch=branch,\n        environment=environment,\n        author=author,\n        commit_sha=commit_sha,\n        timestamp=timestamp\n    )\n    \n    text_content = f'''\nWorkflow {status.title()}: {workflow_name}\n\nDetails:\n- Status: {status.title()}\n- Branch: {branch}\n- Environment: {environment}\n- Author: {author}\n- Commit: {commit_sha}\n- Timestamp: {timestamp}\n    '''\n    \n    subject = f'[GitHub Actions] {workflow_name} - {status.title()}'\n    \nelif notification_type == 'daily_summary':\n    workflow_data = json.loads(os.environ.get('WORKFLOW_DATA', '{}'))\n    total_runs = workflow_data.get('total_runs', 0)\n    success_rate = (workflow_data.get('successful_runs', 0) / total_runs * 100) if total_runs > 0 else 0\n    \n    # Get top workflows\n    workflows = workflow_data.get('workflows', {})\n    top_workflows = sorted(workflows.items(), key=lambda x: sum(x[1].values()), reverse=True)[:5]\n    \n    title = f'Daily CI/CD Summary - {success_rate:.1f}% Success Rate'\n    subtitle = f'Total Runs: {total_runs} | Date: {datetime.now().strftime(\"%Y-%m-%d\")}'\n    \n    html_content = template.render(\n        title=title,\n        subtitle=subtitle,\n        notification_type=notification_type,\n        workflow_data=workflow_data,\n        top_workflows=top_workflows,\n        timestamp=timestamp\n    )\n    \n    text_content = f'''\nDaily CI/CD Summary\n\nStatistics:\n- Total Runs: {workflow_data.get('total_runs', 0)}\n- Successful: {workflow_data.get('successful_runs', 0)}\n- Failed: {workflow_data.get('failed_runs', 0)}\n- Cancelled: {workflow_data.get('cancelled_runs', 0)}\n- Success Rate: {success_rate:.1f}%\n\nGenerated at: {timestamp}\n    '''\n    \n    subject = f'[GitHub Actions] Daily Summary - {datetime.now().strftime(\"%Y-%m-%d\")}'\n\nelse:\n    subject = f'[GitHub Actions] Notification - {notification_type}'\n    html_content = f'<p>Unknown notification type: {notification_type}</p>'\n    text_content = f'Unknown notification type: {notification_type}'\n\nsend_email(subject, html_content, text_content)\nprint('Email notification sent successfully')\nEOF\n\npython send_email.py"
    },
    {
      "file": "notifications.yml",
      "path": "jobs.update-pr-status.steps[0].run",
      "scriptPath": "update_pr.py",
      "command": "# Create PR comment with workflow status\ncat > update_pr.py << 'EOF'\nimport requests\nimport os\nimport json\n\ndef get_status_emoji(status):\n    if status == 'success':\n        return 'âœ…'\n    elif status == 'failure':\n        return 'âŒ'\n    elif status == 'cancelled':\n        return 'âš ï¸'\n    else:\n        return 'â“'\n\n# Get environment variables\ntoken = os.environ['GITHUB_TOKEN']\nrepo = os.environ['GITHUB_REPOSITORY']\npr_number = os.environ['PR_NUMBER']\nworkflow_name = os.environ['WORKFLOW_NAME']\nworkflow_status = os.environ['WORKFLOW_STATUS']\ncommit_sha = os.environ['COMMIT_SHA'][:8]\n\nheaders = {\n    'Authorization': f'token {token}',\n    'Accept': 'application/vnd.github.v3+json'\n}\n\n# Create comment\nstatus_emoji = get_status_emoji(workflow_status)\ncomment_body = f'''\n## {status_emoji} Workflow Update: {workflow_name}\n\n**Status:** {status_emoji} {workflow_status.title()}\n**Commit:** `{commit_sha}`\n**Workflow:** {workflow_name}\n\n<details>\n<summary>View workflow details</summary>\n\n- **Repository:** {repo}\n- **Branch:** {os.environ.get('GITHUB_HEAD_REF', 'unknown')}\n- **Triggered by:** {os.environ.get('GITHUB_ACTOR', 'unknown')}\n- **Run ID:** {os.environ.get('GITHUB_RUN_ID', 'unknown')}\n\n[View full workflow run](https://github.com/{repo}/actions/runs/{os.environ.get('GITHUB_RUN_ID', '')})\n</details>\n'''\n\n# Post comment\nurl = f'https://api.github.com/repos/{repo}/issues/{pr_number}/comments'\ndata = {'body': comment_body}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nif response.status_code == 201:\n    print('PR comment posted successfully')\nelse:\n    print(f'Failed to post PR comment: {response.status_code} - {response.text}')\nEOF\n\npython update_pr.py"
    },
    {
      "file": "notifications.yml",
      "path": "jobs.generate-status-badges.steps[1].run",
      "scriptPath": "generate_badges.py",
      "command": "# Create badge generation script\ncat > generate_badges.py << 'EOF'\nimport json\nimport os\nimport requests\nfrom urllib.parse import quote\n\ndef generate_badge_url(label, message, color):\n    label_encoded = quote(label)\n    message_encoded = quote(message)\n    return f'https://img.shields.io/badge/{label_encoded}-{message_encoded}-{color}'\n\ndef get_color_for_percentage(percentage):\n    if percentage >= 90:\n        return 'brightgreen'\n    elif percentage >= 70:\n        return 'yellow'\n    else:\n        return 'red'\n\n# Load workflow data\nworkflow_data = json.loads(os.environ.get('WORKFLOW_DATA', '{}'))\n\ntotal_runs = workflow_data.get('total_runs', 0)\nsuccessful_runs = workflow_data.get('successful_runs', 0)\n\nsuccess_rate = (successful_runs / total_runs * 100) if total_runs > 0 else 0\ncolor = get_color_for_percentage(success_rate)\n\n# Generate badges\nbadges = {\n    'build_status': generate_badge_url('Build', 'Passing' if success_rate >= 90 else 'Failing', 'brightgreen' if success_rate >= 90 else 'red'),\n    'success_rate': generate_badge_url('Success Rate', f'{success_rate:.1f}%', color),\n    'total_runs': generate_badge_url('Total Runs', str(total_runs), 'blue'),\n    'last_updated': generate_badge_url('Last Updated', 'Today', 'blue')\n}\n\n# Create README badges section\nbadges_markdown = f'''\n## Status Badges\n\n![Build Status]({badges['build_status']})\n![Success Rate]({badges['success_rate']})\n![Total Runs]({badges['total_runs']})\n![Last Updated]({badges['last_updated']})\n'''\n\nprint('Generated status badges:')\nprint(badges_markdown)\n\n# Save badges data\nwith open('badges.json', 'w') as f:\n    json.dump(badges, f, indent=2)\n\nwith open('badges.md', 'w') as f:\n    f.write(badges_markdown)\nEOF\n\npython generate_badges.py\n\n# Upload badges as artifacts\necho \"Badges generated successfully\""
    },
    {
      "file": "performance-monitoring.yml",
      "path": "jobs.backend-performance-tests.steps[5].run",
      "scriptPath": "script.py",
      "command": "cat > performance-test-${{ matrix.service }}.py << 'EOF'\nimport time\nimport json\nimport statistics\nimport requests\nimport psutil\nimport sys\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nclass PerformanceTester:\n    def __init__(self, base_url, service_name, test_duration, load_pattern):\n        self.base_url = base_url\n        self.service_name = service_name\n        self.test_duration = int(test_duration) * 60  # Convert to seconds\n        self.load_pattern = load_pattern\n        self.results = {\n            'service': service_name,\n            'test_duration': test_duration,\n            'load_pattern': load_pattern,\n            'timestamp': datetime.utcnow().isoformat(),\n            'metrics': {}\n        }\n\n    def get_load_config(self):\n        configs = {\n            'baseline': {'concurrent_users': 10, 'requests_per_user': 100},\n            'stress': {'concurrent_users': 50, 'requests_per_user': 200},\n            'spike': {'concurrent_users': 100, 'requests_per_user': 50},\n            'endurance': {'concurrent_users': 20, 'requests_per_user': 500}\n        }\n        return configs.get(self.load_pattern, configs['baseline'])\n\n    def make_request(self, endpoint='/health'):\n        start_time = time.time()\n        try:\n            response = requests.get(f\"{self.base_url}{endpoint}\", timeout=30)\n            end_time = time.time()\n            return {\n                'status_code': response.status_code,\n                'response_time': (end_time - start_time) * 1000,  # ms\n                'success': response.status_code == 200,\n                'timestamp': start_time\n            }\n        except Exception as e:\n            end_time = time.time()\n            return {\n                'status_code': 0,\n                'response_time': (end_time - start_time) * 1000,\n                'success': False,\n                'error': str(e),\n                'timestamp': start_time\n            }\n\n    def run_load_test(self):\n        config = self.get_load_config()\n        concurrent_users = config['concurrent_users']\n        requests_per_user = config['requests_per_user']\n        \n        print(f\"Starting load test for {self.service_name}\")\n        print(f\"Configuration: {concurrent_users} users, {requests_per_user} requests each\")\n        \n        all_results = []\n        start_time = time.time()\n        \n        def user_session(user_id):\n            session_results = []\n            for i in range(requests_per_user):\n                if time.time() - start_time > self.test_duration:\n                    break\n                result = self.make_request()\n                session_results.append(result)\n                time.sleep(0.1)  # Small delay between requests\n            return session_results\n        \n        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n            futures = [executor.submit(user_session, i) for i in range(concurrent_users)]\n            \n            for future in as_completed(futures):\n                try:\n                    results = future.result()\n                    all_results.extend(results)\n                except Exception as e:\n                    print(f\"Error in user session: {e}\")\n        \n        return all_results\n\n    def analyze_results(self, results):\n        if not results:\n            return {'error': 'No results to analyze'}\n        \n        successful_requests = [r for r in results if r['success']]\n        failed_requests = [r for r in results if not r['success']]\n        \n        response_times = [r['response_time'] for r in successful_requests]\n        \n        if not response_times:\n            return {'error': 'No successful requests'}\n        \n        total_requests = len(results)\n        success_rate = len(successful_requests) / total_requests * 100\n        \n        # Calculate percentiles\n        response_times.sort()\n        p50 = statistics.median(response_times)\n        p95 = response_times[int(0.95 * len(response_times))] if len(response_times) > 1 else response_times[0]\n        p99 = response_times[int(0.99 * len(response_times))] if len(response_times) > 1 else response_times[0]\n        \n        # Calculate throughput\n        test_duration_actual = max(r['timestamp'] for r in results) - min(r['timestamp'] for r in results)\n        throughput = len(successful_requests) / test_duration_actual if test_duration_actual > 0 else 0\n        \n        return {\n            'total_requests': total_requests,\n            'successful_requests': len(successful_requests),\n            'failed_requests': len(failed_requests),\n            'success_rate': round(success_rate, 2),\n            'response_time': {\n                'mean': round(statistics.mean(response_times), 2),\n                'median': round(p50, 2),\n                'p95': round(p95, 2),\n                'p99': round(p99, 2),\n                'min': round(min(response_times), 2),\n                'max': round(max(response_times), 2)\n            },\n            'throughput': round(throughput, 2),\n            'test_duration': round(test_duration_actual, 2)\n        }\n\n    def get_system_metrics(self):\n        return {\n            'cpu_percent': psutil.cpu_percent(interval=1),\n            'memory_percent': psutil.virtual_memory().percent,\n            'disk_usage': psutil.disk_usage('/').percent\n        }\n\n    def run_test(self):\n        print(f\"Starting performance test for {self.service_name}\")\n        \n        # Get initial system metrics\n        initial_metrics = self.get_system_metrics()\n        \n        # Run load test\n        results = self.run_load_test()\n        \n        # Get final system metrics\n        final_metrics = self.get_system_metrics()\n        \n        # Analyze results\n        analysis = self.analyze_results(results)\n        \n        self.results['metrics'] = analysis\n        self.results['system_metrics'] = {\n            'initial': initial_metrics,\n            'final': final_metrics\n        }\n        \n        return self.results\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 5:\n        print(\"Usage: python script.py <base_url> <service_name> <test_duration> <load_pattern>\")\n        sys.exit(1)\n    \n    base_url = sys.argv[1]\n    service_name = sys.argv[2]\n    test_duration = sys.argv[3]\n    load_pattern = sys.argv[4]\n    \n    tester = PerformanceTester(base_url, service_name, test_duration, load_pattern)\n    results = tester.run_test()\n    \n    # Save results to file\n    with open(f'performance-results-{service_name}.json', 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    print(f\"Performance test completed for {service_name}\")\n    print(f\"Results saved to performance-results-{service_name}.json\")\nEOF"
    },
    {
      "file": "performance-monitoring.yml",
      "path": "jobs.backend-performance-tests.steps[6].run",
      "scriptPath": "performance-test-$SERVICE.py",
      "command": "SERVICE=\"${{ matrix.service }}\"\nSERVICE_URL=\"${{ steps.service-config.outputs.service_url }}\"\nTEST_DURATION=\"${{ needs.setup-performance-environment.outputs.test_duration }}\"\nLOAD_PATTERN=\"${{ needs.setup-performance-environment.outputs.load_pattern }}\"\n\necho \"Running performance test for $SERVICE\"\necho \"URL: $SERVICE_URL\"\necho \"Duration: $TEST_DURATION minutes\"\necho \"Load pattern: $LOAD_PATTERN\"\n\n# Run the performance test\npython performance-test-$SERVICE.py \"$SERVICE_URL\" \"$SERVICE\" \"$TEST_DURATION\" \"$LOAD_PATTERN\""
    },
    {
      "file": "performance-monitoring.yml",
      "path": "jobs.frontend-performance-tests.steps[5].run",
      "scriptPath": "extract-lighthouse-metrics.js",
      "command": "APP=\"${{ matrix.app }}\"\nAPP_URL=\"${{ steps.app-config.outputs.app_url }}\"\n\necho \"Running Lighthouse performance test for $APP\"\necho \"URL: $APP_URL\"\n\n# Run Lighthouse CI\nlhci autorun\n\n# Extract key metrics and create summary\ncat > extract-lighthouse-metrics.js << 'EOF'\nconst fs = require('fs');\nconst path = require('path');\n\nconst resultsDir = './lighthouse-results';\nconst files = fs.readdirSync(resultsDir).filter(f => f.endsWith('.json'));\n\nif (files.length === 0) {\n  console.error('No Lighthouse results found');\n  process.exit(1);\n}\n\nconst results = files.map(file => {\n  const content = fs.readFileSync(path.join(resultsDir, file), 'utf8');\n  return JSON.parse(content);\n});\n\n// Calculate averages\nconst avgMetrics = {\n  performance: 0,\n  accessibility: 0,\n  bestPractices: 0,\n  seo: 0,\n  firstContentfulPaint: 0,\n  largestContentfulPaint: 0,\n  cumulativeLayoutShift: 0,\n  speedIndex: 0,\n  totalBlockingTime: 0\n};\n\nresults.forEach(result => {\n  avgMetrics.performance += result.categories.performance.score;\n  avgMetrics.accessibility += result.categories.accessibility.score;\n  avgMetrics.bestPractices += result.categories['best-practices'].score;\n  avgMetrics.seo += result.categories.seo.score;\n  \n  const audits = result.audits;\n  avgMetrics.firstContentfulPaint += audits['first-contentful-paint'].numericValue;\n  avgMetrics.largestContentfulPaint += audits['largest-contentful-paint'].numericValue;\n  avgMetrics.cumulativeLayoutShift += audits['cumulative-layout-shift'].numericValue;\n  avgMetrics.speedIndex += audits['speed-index'].numericValue;\n  avgMetrics.totalBlockingTime += audits['total-blocking-time'].numericValue;\n});\n\nconst count = results.length;\nObject.keys(avgMetrics).forEach(key => {\n  avgMetrics[key] = Math.round((avgMetrics[key] / count) * 100) / 100;\n});\n\nconst summary = {\n  app: process.argv[2],\n  url: process.argv[3],\n  timestamp: new Date().toISOString(),\n  numberOfRuns: count,\n  metrics: {\n    lighthouse: {\n      performance: avgMetrics.performance,\n      accessibility: avgMetrics.accessibility,\n      bestPractices: avgMetrics.bestPractices,\n      seo: avgMetrics.seo\n    },\n    coreWebVitals: {\n      firstContentfulPaint: avgMetrics.firstContentfulPaint,\n      largestContentfulPaint: avgMetrics.largestContentfulPaint,\n      cumulativeLayoutShift: avgMetrics.cumulativeLayoutShift,\n      speedIndex: avgMetrics.speedIndex,\n      totalBlockingTime: avgMetrics.totalBlockingTime\n    }\n  }\n};\n\nfs.writeFileSync(`frontend-performance-${process.argv[2]}.json`, JSON.stringify(summary, null, 2));\nconsole.log(`Frontend performance results saved for ${process.argv[2]}`);\nEOF\n\nnode extract-lighthouse-metrics.js \"$APP\" \"$APP_URL\""
    },
    {
      "file": "performance-monitoring.yml",
      "path": "jobs.analyze-performance-results.steps[6].run",
      "scriptPath": "analyze-performance.py",
      "command": "BASELINE_COMPARISON=\"${{ needs.setup-performance-environment.outputs.baseline_comparison }}\"\n\necho \"Running performance analysis...\"\necho \"Baseline comparison: $BASELINE_COMPARISON\"\n\npython analyze-performance.py performance-results/ \"$BASELINE_COMPARISON\""
    },
    {
      "file": "performance-monitoring.yml",
      "path": "jobs.analyze-performance-results.steps[7].run",
      "scriptPath": "create-performance-report.py",
      "command": "cat > create-performance-report.py << 'EOF'\nimport json\nimport os\nfrom datetime import datetime\n\ndef create_markdown_report(analysis_file):\n    with open(analysis_file, 'r') as f:\n        analysis = json.load(f)\n    \n    report = []\n    report.append(\"# ðŸš€ Performance Monitoring Report\")\n    report.append(f\"**Generated:** {analysis['timestamp']}\")\n    report.append(\"\")\n    \n    # Summary\n    summary = analysis['summary']\n    report.append(\"## ðŸ“Š Summary\")\n    report.append(f\"- **Backend Services Tested:** {summary.get('backend_services', 0)}\")\n    report.append(f\"- **Frontend Apps Tested:** {summary.get('frontend_apps', 0)}\")\n    report.append(f\"- **E2E Scenarios Tested:** {summary.get('e2e_scenarios', 0)}\")\n    report.append(f\"- **Performance Regressions:** {summary.get('total_regressions', 0)}\")\n    report.append(f\"- **Performance Improvements:** {summary.get('total_improvements', 0)}\")\n    \n    if 'avg_backend_p95' in summary:\n        report.append(f\"- **Average Backend P95:** {summary['avg_backend_p95']}ms\")\n    if 'avg_frontend_performance' in summary:\n        report.append(f\"- **Average Frontend Performance:** {summary['avg_frontend_performance']}\")\n    \n    report.append(\"\")\n    \n    # Regressions\n    if analysis['regressions']:\n        report.append(\"## ðŸ”´ Performance Regressions\")\n        report.append(\"| Component | Metric | Current | Baseline | Change |\")\n        report.append(\"|-----------|--------|---------|----------|--------|\")\n        for reg in analysis['regressions']:\n            report.append(f\"| {reg['component']} | {reg['metric']} | {reg['current']} | {reg['baseline']} | {reg['change_percent']:+.1f}% |\")\n        report.append(\"\")\n    \n    # Improvements\n    if analysis['improvements']:\n        report.append(\"## ðŸŸ¢ Performance Improvements\")\n        report.append(\"| Component | Metric | Current | Baseline | Change |\")\n        report.append(\"|-----------|--------|---------|----------|--------|\")\n        for imp in analysis['improvements']:\n            report.append(f\"| {imp['component']} | {imp['metric']} | {imp['current']} | {imp['baseline']} | {imp['change_percent']:+.1f}% |\")\n        report.append(\"\")\n    \n    # Recommendations\n    if analysis['recommendations']:\n        report.append(\"## ðŸ’¡ Recommendations\")\n        for rec in analysis['recommendations']:\n            report.append(f\"- {rec}\")\n        report.append(\"\")\n    \n    # Status\n    if analysis['regressions']:\n        report.append(\"## âš ï¸ Status: Performance Issues Detected\")\n        report.append(\"Please review the regressions above and take appropriate action.\")\n    else:\n        report.append(\"## âœ… Status: All Performance Metrics Within Acceptable Range\")\n        report.append(\"No significant performance regressions detected.\")\n    \n    return \"\\n\".join(report)\n\nif __name__ == \"__main__\":\n    report_content = create_markdown_report('performance-analysis.json')\n    \n    with open('performance-report.md', 'w') as f:\n        f.write(report_content)\n    \n    print(\"Performance report created: performance-report.md\")\nEOF\n\npython create-performance-report.py"
    },
    {
      "file": "test.yml",
      "path": "jobs.backend-unit-tests.steps[3].run",
      "scriptPath": "manage.py",
      "command": "python manage.py migrate --settings=config.settings.test"
    },
    {
      "file": "test.yml",
      "path": "jobs.integration-tests.steps[5].run",
      "scriptPath": "scripts/setup-test-db.py",
      "command": "python scripts/setup-test-db.py"
    },
    {
      "file": "test.yml",
      "path": "jobs.integration-tests.steps[6].run",
      "scriptPath": "apps/community/server/main.py",
      "command": "# å¯åŠ¨åŽç«¯æœåŠ¡\npython apps/community/server/main.py &\npython apps/community/oidc-server/main.py &\n\n# ç­‰å¾…æœåŠ¡å¯åŠ¨\nsleep 10\n\n# å¥åº·æ£€æŸ¥\ncurl -f http://localhost:8000/health\ncurl -f http://localhost:8001/health"
    },
    {
      "file": "test.yml",
      "path": "jobs.integration-tests.steps[6].run",
      "scriptPath": "apps/community/oidc-server/main.py",
      "command": "# å¯åŠ¨åŽç«¯æœåŠ¡\npython apps/community/server/main.py &\npython apps/community/oidc-server/main.py &\n\n# ç­‰å¾…æœåŠ¡å¯åŠ¨\nsleep 10\n\n# å¥åº·æ£€æŸ¥\ncurl -f http://localhost:8000/health\ncurl -f http://localhost:8001/health"
    },
    {
      "file": "test.yml",
      "path": "jobs.e2e-tests.steps[6].run",
      "scriptPath": "scripts/setup-test-db.py",
      "command": "python scripts/setup-test-db.py"
    },
    {
      "file": "test.yml",
      "path": "jobs.e2e-tests.steps[7].run",
      "scriptPath": "apps/community/server/main.py",
      "command": "# å¯åŠ¨åŽç«¯æœåŠ¡\npython apps/community/server/main.py &\npython apps/community/oidc-server/main.py &\n\n# æž„å»ºå¹¶å¯åŠ¨å‰ç«¯\ncd apps/community/admin\npnpm build\npnpm preview &\n\n# ç­‰å¾…æœåŠ¡å¯åŠ¨\nsleep 15\n\n# å¥åº·æ£€æŸ¥\ncurl -f http://localhost:8000/health\ncurl -f http://localhost:3000"
    },
    {
      "file": "test.yml",
      "path": "jobs.e2e-tests.steps[7].run",
      "scriptPath": "apps/community/oidc-server/main.py",
      "command": "# å¯åŠ¨åŽç«¯æœåŠ¡\npython apps/community/server/main.py &\npython apps/community/oidc-server/main.py &\n\n# æž„å»ºå¹¶å¯åŠ¨å‰ç«¯\ncd apps/community/admin\npnpm build\npnpm preview &\n\n# ç­‰å¾…æœåŠ¡å¯åŠ¨\nsleep 15\n\n# å¥åº·æ£€æŸ¥\ncurl -f http://localhost:8000/health\ncurl -f http://localhost:3000"
    },
    {
      "file": "test.yml",
      "path": "jobs.performance-tests.steps[6].run",
      "scriptPath": "scripts/setup-test-db.py",
      "command": "python scripts/setup-test-db.py --performance"
    },
    {
      "file": "test.yml",
      "path": "jobs.performance-tests.steps[7].run",
      "scriptPath": "apps/community/server/main.py",
      "command": "# å¯åŠ¨åŽç«¯æœåŠ¡ (ç”Ÿäº§æ¨¡å¼)\nexport NODE_ENV=production\npython apps/community/server/main.py &\npython apps/community/oidc-server/main.py &\n\n# ç­‰å¾…æœåŠ¡å¯åŠ¨\nsleep 10\n\n# å¥åº·æ£€æŸ¥\ncurl -f http://localhost:8000/health"
    },
    {
      "file": "test.yml",
      "path": "jobs.performance-tests.steps[7].run",
      "scriptPath": "apps/community/oidc-server/main.py",
      "command": "# å¯åŠ¨åŽç«¯æœåŠ¡ (ç”Ÿäº§æ¨¡å¼)\nexport NODE_ENV=production\npython apps/community/server/main.py &\npython apps/community/oidc-server/main.py &\n\n# ç­‰å¾…æœåŠ¡å¯åŠ¨\nsleep 10\n\n# å¥åº·æ£€æŸ¥\ncurl -f http://localhost:8000/health"
    },
    {
      "file": "test.yml",
      "path": "jobs.performance-tests.steps[8].run",
      "scriptPath": "./run-performance-tests.sh",
      "command": "cd tests/performance\nchmod +x run-performance-tests.sh\n./run-performance-tests.sh --type=all --duration=5m --vus=50"
    },
    {
      "file": "test.yml",
      "path": "jobs.performance-tests.steps[8].run",
      "scriptPath": "run-performance-tests.sh",
      "command": "cd tests/performance\nchmod +x run-performance-tests.sh\n./run-performance-tests.sh --type=all --duration=5m --vus=50"
    },
    {
      "file": "test.yml",
      "path": "jobs.performance-tests.steps[9].run",
      "scriptPath": "scripts/generate-report.js",
      "command": "cd tests/performance\nnode scripts/generate-report.js --html --json"
    },
    {
      "file": "test.yml",
      "path": "jobs.performance-tests.steps[10].run",
      "scriptPath": "scripts/update-benchmarks.js",
      "command": "cd tests/performance\nnode scripts/update-benchmarks.js\n\n# æäº¤åŸºå‡†æ›´æ–°\ngit config --local user.email \"action@github.com\"\ngit config --local user.name \"GitHub Action\"\ngit add benchmarks.json\ngit diff --staged --quiet || git commit -m \"chore: æ›´æ–°æ€§èƒ½åŸºå‡†"
    },
    {
      "file": "update-badges.yml",
      "path": "jobs.generate-badges.steps[3].run",
      "scriptPath": ".github/scripts/generate-badges.py",
      "command": "python .github/scripts/generate-badges.py \\\n  --config .github/config/badges.yml \\\n  --badge-types \"${{ needs.prepare-badge-update.outputs.badge_types }}\" \\\n  --output-dir badges \\\n  --coverage-data '${{ needs.collect-badge-data.outputs.coverage_data }}' \\\n  --performance-data '${{ needs.collect-badge-data.outputs.performance_data }}' \\\n  --security-data '${{ needs.collect-badge-data.outputs.security_data }}' \\\n  --deployment-data '${{ needs.collect-badge-data.outputs.deployment_data }}' \\\n  --github-token \"$GITHUB_TOKEN\" \\\n  --repository \"${{ github.repository }}\" \\\n  --verbose"
    }
  ],
  "isValid": false
}